"""Test Validation System - Core Validator.

This module implements the TestValidator class that validates and executes tests
generated by the TesterAgent, ensuring they are syntactically correct, executable,
and effective at preventing disasters like PR #23.
"""

import os
import sys
import json
import tempfile
import subprocess
import shutil
import ast
import re
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
from datetime import datetime

from .models import (
    TestStatus, TestResult, TestValidationResult, ValidationIssue,
    ValidationIssueType, TestEnvironment, ValidationMetrics
)
from core.logging import get_logger

logger = get_logger(__name__)


class TestValidator:
    """
    Validates and executes tests generated by Tester Agent.
    
    Ensures tests are syntactically correct, executable, and effective
    at catching the issues they're designed to prevent, specifically
    preventing disasters like PR #23.
    
    Key Features:
    - Syntax validation for multiple testing frameworks
    - Test execution with proper isolation
    - Coverage analysis and reporting
    - Disaster prevention validation (PR #23 protection)
    - Performance monitoring and optimization
    - Comprehensive error handling and logging
    """
    
    def __init__(
        self,
        default_timeout: int = 300,
        max_execution_time: float = 600.0,
        enable_coverage: bool = True,
        strict_validation: bool = True
    ):
        """
        Initialize the Test Validator.
        
        Args:
            default_timeout: Default timeout for test execution in seconds
            max_execution_time: Maximum time allowed for validation in seconds
            enable_coverage: Whether to collect code coverage data
            strict_validation: Whether to use strict validation rules
        """
        self.default_timeout = default_timeout
        self.max_execution_time = max_execution_time
        self.enable_coverage = enable_coverage
        self.strict_validation = strict_validation
        
        # Supported testing frameworks and their configurations
        self.framework_configs = {
            'pytest': {
                'test_file_pattern': 'test_*.py',
                'test_function_prefix': 'test_',
                'imports': ['import pytest', 'from unittest.mock import Mock, patch, MagicMock'],
                'assertion_patterns': ['assert ', 'pytest.raises'],
                'install_command': 'pip install pytest pytest-cov pytest-json-report pytest-timeout',
                'executable': 'python -m pytest',
                'config_file': 'pytest.ini'
            },
            'jest': {
                'test_file_pattern': '*.test.js',
                'test_function_prefix': 'test(',
                'imports': [],
                'assertion_patterns': ['expect(', '.toBe(', '.toEqual(', '.toThrow('],
                'install_command': 'npm install jest jest-json-reporter',
                'executable': 'npx jest',
                'config_file': 'jest.config.js'
            },
            'unittest': {
                'test_file_pattern': 'test_*.py',
                'test_function_prefix': 'test_',
                'imports': ['import unittest', 'from unittest.mock import Mock, patch, MagicMock'],
                'assertion_patterns': ['self.assert', 'self.assertEqual', 'self.assertTrue'],
                'install_command': 'pip install coverage',
                'executable': 'python -m unittest',
                'config_file': '.coveragerc'
            }
        }
        
        # Metrics tracking
        self.metrics = ValidationMetrics()
        
        # Disaster prevention patterns for PR #23 protection
        self.disaster_prevention_patterns = {
            'file_integrity_checks': [
                r'os\.path\.exists',
                r'file.*exists',
                r'len\(.*\)\s*>\s*\d+',
                r'file.*size',
                r'file.*length'
            ],
            'content_preservation': [
                r'preserve',
                r'unchanged',
                r'original.*content',
                r'specific.*phrase',
                r'targeted.*change',
                r'only.*remove'
            ],
            'edge_case_protection': [
                r'empty',
                r'null',
                r'none',
                r'zero',
                r'boundary',
                r'maximum',
                r'minimum',
                r'invalid'
            ],
            'error_handling': [
                r'raises',
                r'exception',
                r'error',
                r'try.*except',
                r'assertRaises'
            ]
        }
        
        logger.info(f"TestValidator initialized: timeout={default_timeout}s, coverage={enable_coverage}")
    
    async def validate_test_suite(
        self,
        test_artifacts: List[Dict[str, Any]],
        target_code: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> TestValidationResult:
        """
        Validate complete test suite.
        
        Args:
            test_artifacts: Test files generated by Tester Agent
            target_code: Code being tested (for coverage analysis)
            context: Additional context for validation
            
        Returns:
            Complete validation result with all metrics and analysis
        """
        start_time = datetime.utcnow()
        
        logger.info(f"Starting test suite validation: {len(test_artifacts)} artifacts")
        
        try:
            # Initialize validation result
            validation_result = TestValidationResult(
                overall_success=False,
                total_tests=0,
                passed_tests=0,
                failed_tests=0,
                error_tests=0,
                skipped_tests=0,
                total_execution_time=0.0,
                average_test_time=0.0
            )
            
            # Filter test artifacts
            test_files = [
                artifact for artifact in test_artifacts 
                if artifact.get("type") == "test"
            ]
            
            if not test_files:
                logger.warning("No test files found in artifacts")
                validation_result.validation_issues.append(
                    ValidationIssue(
                        type=ValidationIssueType.QUALITY_ISSUE,
                        severity="critical",
                        message="No test files provided for validation",
                        suggestion="Generate test files before validation"
                    )
                )
                return validation_result
            
            # Detect framework
            framework = self._detect_framework(test_files)
            validation_result.framework_detected = framework
            
            logger.info(f"Detected testing framework: {framework}")
            
            # Create temporary test environment
            with tempfile.TemporaryDirectory() as temp_dir:
                logger.debug(f"Created temporary test environment: {temp_dir}")
                
                # Setup test environment
                test_env = await self._setup_test_environment(
                    test_files, target_code, temp_dir, framework, context
                )
                
                # Step 1: Validate test syntax
                syntax_issues = await self._validate_test_syntax(test_files)
                validation_result.validation_issues.extend(syntax_issues)
                
                # Step 2: Validate test imports and dependencies
                dependency_issues = await self._validate_dependencies(test_env)
                validation_result.validation_issues.extend(dependency_issues)
                
                # Step 3: Execute tests if syntax is valid
                if not any(issue.severity == "critical" for issue in syntax_issues):
                    test_results = await self._execute_tests(test_env)
                    validation_result.test_results = test_results
                    
                    # Update test counts
                    validation_result.total_tests = len(test_results)
                    validation_result.passed_tests = len([r for r in test_results if r.status == TestStatus.PASSED])
                    validation_result.failed_tests = len([r for r in test_results if r.status == TestStatus.FAILED])
                    validation_result.error_tests = len([r for r in test_results if r.status == TestStatus.ERROR])
                    validation_result.skipped_tests = len([r for r in test_results if r.status == TestStatus.SKIPPED])
                    
                    # Calculate timing metrics
                    if test_results:
                        validation_result.total_execution_time = sum(r.execution_time for r in test_results)
                        validation_result.average_test_time = validation_result.total_execution_time / len(test_results)
                else:
                    logger.warning("Skipping test execution due to critical syntax errors")
                
                # Step 4: Analyze coverage if enabled
                if self.enable_coverage and validation_result.test_results:
                    coverage_data = await self._analyze_coverage(test_env)
                    validation_result.overall_coverage_percentage = coverage_data.get('overall', 0.0)
                    validation_result.statement_coverage = coverage_data.get('statement', 0.0)
                    validation_result.branch_coverage = coverage_data.get('branch', 0.0)
                    validation_result.function_coverage = coverage_data.get('function', 0.0)
                
                # Step 5: Validate test effectiveness (disaster prevention)
                effectiveness_issues = await self._validate_test_effectiveness(
                    test_files, context
                )
                validation_result.validation_issues.extend(effectiveness_issues)
                
                # Step 6: Calculate quality and disaster prevention scores
                validation_result.quality_score = self._calculate_quality_score(validation_result)
                validation_result.disaster_prevention_score = self._calculate_disaster_prevention_score(
                    test_files, validation_result
                )
                
                # Step 7: Generate recommendations
                validation_result.recommendations = self._generate_recommendations(validation_result)
                
                # Step 8: Determine overall success
                validation_result.overall_success = self._determine_overall_success(validation_result)
                
                # Add environment info
                validation_result.python_version = sys.version.split()[0]
                validation_result.dependencies_validated = len(dependency_issues) == 0
            
            # Update metrics
            self.metrics.update_with_result(validation_result)
            
            # Log final result
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            status = "SUCCESS" if validation_result.overall_success else "FAILED"
            
            logger.info(
                f"Test validation {status}: {validation_result.total_tests} tests, "
                f"quality={validation_result.quality_score:.1f}, "
                f"disaster_prevention={validation_result.disaster_prevention_score:.1f}, "
                f"time={execution_time:.2f}s"
            )
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Test validation failed with exception: {e}", exc_info=True)
            
            # Create error result
            error_result = TestValidationResult(
                overall_success=False,
                total_tests=0,
                passed_tests=0,
                failed_tests=0,
                error_tests=0,
                skipped_tests=0,
                total_execution_time=(datetime.utcnow() - start_time).total_seconds(),
                average_test_time=0.0,
                validation_issues=[
                    ValidationIssue(
                        type=ValidationIssueType.EXECUTION_ERROR,
                        severity="critical",
                        message=f"Validation system error: {str(e)}",
                        suggestion="Check validation system configuration and logs"
                    )
                ],
                recommendations=["Fix validation system issues before proceeding"]
            )
            
            self.metrics.update_with_result(error_result)
            return error_result
    
    def _detect_framework(self, test_files: List[Dict[str, Any]]) -> str:
        """Detect testing framework from test files."""
        framework_indicators = {
            'pytest': ['import pytest', 'pytest.', '@pytest.', 'def test_'],
            'jest': ['test(', 'describe(', 'expect(', 'it(', '.test.js'],
            'unittest': ['import unittest', 'class.*unittest.TestCase', 'def test_.*self']
        }
        
        framework_scores = {name: 0 for name in framework_indicators}
        
        for test_file in test_files:
            content = test_file.get('content', '')
            path = test_file.get('path', '')
            
            for framework, indicators in framework_indicators.items():
                for indicator in indicators:
                    if re.search(indicator, content, re.IGNORECASE):
                        framework_scores[framework] += 1
                    if indicator in path:
                        framework_scores[framework] += 1
        
        # Return framework with highest score, default to pytest
        detected_framework = max(framework_scores.items(), key=lambda x: x[1])[0]
        
        if framework_scores[detected_framework] == 0:
            logger.info("No clear framework detected, defaulting to pytest")
            return 'pytest'
        
        logger.debug(f"Framework detection scores: {framework_scores}")
        return detected_framework
    
    async def _setup_test_environment(
        self,
        test_files: List[Dict[str, Any]],
        target_code: Optional[str],
        temp_dir: str,
        framework: str,
        context: Optional[Dict[str, Any]]
    ) -> TestEnvironment:
        """Setup temporary test environment with all necessary files."""
        
        # Create directory structure
        test_dir = Path(temp_dir) / "tests"
        src_dir = Path(temp_dir) / "src"
        test_dir.mkdir(parents=True, exist_ok=True)
        src_dir.mkdir(parents=True, exist_ok=True)
        
        # Write test files
        for test_file in test_files:
            test_path = test_dir / Path(test_file["path"]).name
            test_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(test_path, "w", encoding="utf-8") as f:
                f.write(test_file["content"])
        
        # Write target code if provided
        if target_code:
            src_path = src_dir / "main.py"
            with open(src_path, "w", encoding="utf-8") as f:
                f.write(target_code)
        
        # Create framework-specific configuration
        await self._create_framework_config(temp_dir, framework)
        
        # Create requirements file
        await self._create_requirements_file(temp_dir, framework)
        
        # Create test environment object
        return TestEnvironment(
            temp_directory=temp_dir,
            framework=framework,
            timeout_seconds=self.default_timeout,
            required_packages=self._get_required_packages(framework)
        )
    
    async def _create_framework_config(self, temp_dir: str, framework: str) -> None:
        """Create framework-specific configuration files."""
        
        if framework == "pytest":
            config_content = """[tool:pytest]
testpaths = tests
python_files = test_*.py
python_functions = test_*
python_classes = Test*
addopts = --verbose --tb=short --json-report --json-report-file=test_results.json
timeout = 60
markers =
    unit: Unit tests
    integration: Integration tests
    edge_case: Edge case tests
    error_handling: Error handling tests
    performance: Performance tests
"""
            if self.enable_coverage:
                config_content += "    --cov=src --cov-report=json --cov-report=term\n"
            
            config_path = Path(temp_dir) / "pytest.ini"
            
        elif framework == "jest":
            config_content = """{
  "testEnvironment": "node",
  "testMatch": ["**/tests/**/*.test.js", "**/tests/**/*.spec.js"],
  "collectCoverage": """ + ("true" if self.enable_coverage else "false") + """,
  "collectCoverageFrom": [
    "src/**/*.js",
    "!src/**/*.test.js",
    "!src/**/*.spec.js"
  ],
  "coverageDirectory": "coverage",
  "coverageReporters": ["json", "text"],
  "verbose": true,
  "testTimeout": """ + str(min(self.default_timeout * 1000, 60000)) + """,
  "reporters": [
    "default",
    ["jest-json-reporter", {"outputFile": "test_results.json"}]
  ]
}"""
            config_path = Path(temp_dir) / "jest.config.json"
            
        elif framework == "unittest":
            config_content = """[run]
source = src
omit = 
    */tests/*
    */test_*
    */__pycache__/*

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError

[json]
output = coverage.json
"""
            config_path = Path(temp_dir) / ".coveragerc"
        
        else:
            logger.warning(f"Unknown framework {framework}, skipping config creation")
            return
        
        with open(config_path, "w", encoding="utf-8") as f:
            f.write(config_content)
        
        logger.debug(f"Created {framework} configuration: {config_path}")
    
    async def _create_requirements_file(self, temp_dir: str, framework: str) -> None:
        """Create requirements file for test execution."""
        
        requirements = self._get_required_packages(framework)
        
        req_path = Path(temp_dir) / "requirements.txt"
        with open(req_path, "w", encoding="utf-8") as f:
            f.write("\n".join(requirements))
        
        logger.debug(f"Created requirements file with {len(requirements)} packages")
    
    def _get_required_packages(self, framework: str) -> List[str]:
        """Get required packages for framework."""
        
        base_packages = []
        
        if framework == "pytest":
            base_packages = [
                "pytest>=7.0.0",
                "pytest-json-report>=1.5.0",
                "pytest-timeout>=2.1.0"
            ]
            if self.enable_coverage:
                base_packages.append("pytest-cov>=4.0.0")
                
        elif framework == "unittest":
            base_packages = []
            if self.enable_coverage:
                base_packages.append("coverage>=6.0.0")
                
        elif framework == "jest":
            base_packages = []  # Jest packages handled via npm
        
        return base_packages
    
    async def _validate_test_syntax(self, test_files: List[Dict[str, Any]]) -> List[ValidationIssue]:
        """Validate test files have correct syntax."""
        
        issues = []
        
        for test_file in test_files:
            file_path = test_file.get("path", "unknown")
            content = test_file.get("content", "")
            
            try:
                # Check if it's a Python file
                if file_path.endswith('.py'):
                    # Attempt to parse as Python AST
                    ast.parse(content)
                    
                    # Validate test structure
                    syntax_issues = self._validate_python_test_structure(content, file_path)
                    issues.extend(syntax_issues)
                    
                elif file_path.endswith('.js'):
                    # Basic JavaScript validation (could be enhanced with actual JS parser)
                    js_issues = self._validate_javascript_test_structure(content, file_path)
                    issues.extend(js_issues)
                    
                logger.debug(f"Syntax validation passed for {file_path}")
                
            except SyntaxError as e:
                issues.append(ValidationIssue(
                    type=ValidationIssueType.SYNTAX_ERROR,
                    severity="critical",
                    message=f"Syntax error in {file_path}: {e.msg}",
                    location=f"line {e.lineno}" if hasattr(e, 'lineno') else file_path,
                    suggestion="Fix syntax error before test execution",
                    test_name=file_path
                ))
                
            except Exception as e:
                issues.append(ValidationIssue(
                    type=ValidationIssueType.SYNTAX_ERROR,
                    severity="major",
                    message=f"Validation error in {file_path}: {str(e)}",
                    location=file_path,
                    suggestion="Check file content and structure",
                    test_name=file_path
                ))
        
        logger.info(f"Syntax validation complete: {len(issues)} issues found")
        return issues
    
    def _validate_python_test_structure(self, content: str, file_path: str) -> List[ValidationIssue]:
        """Validate Python test file structure and quality."""
        
        issues = []
        
        try:
            tree = ast.parse(content)
            
            # Find test functions
            test_functions = []
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and node.name.startswith('test_'):
                    test_functions.append(node)
            
            if not test_functions:
                issues.append(ValidationIssue(
                    type=ValidationIssueType.QUALITY_ISSUE,
                    severity="major",
                    message=f"No test functions found in {file_path}",
                    location=file_path,
                    suggestion="Add test functions starting with 'test_'",
                    test_name=file_path
                ))
            
            # Validate each test function
            for func in test_functions:
                func_issues = self._validate_test_function(func, file_path)
                issues.extend(func_issues)
                
        except Exception as e:
            issues.append(ValidationIssue(
                type=ValidationIssueType.SYNTAX_ERROR,
                severity="major",
                message=f"Failed to analyze Python structure in {file_path}: {str(e)}",
                location=file_path,
                suggestion="Check Python syntax and structure"
            ))
        
        return issues
    
    def _validate_test_function(self, func_node: ast.FunctionDef, file_path: str) -> List[ValidationIssue]:
        """Validate individual test function."""
        
        issues = []
        func_name = func_node.name
        
        # Check for assertions
        has_assertions = False
        for node in ast.walk(func_node):
            if isinstance(node, ast.Assert) or \
               (isinstance(node, ast.Call) and 
                isinstance(node.func, ast.Attribute) and 
                node.func.attr.startswith('assert')):
                has_assertions = True
                break
        
        if not has_assertions:
            issues.append(ValidationIssue(
                type=ValidationIssueType.ASSERTION_MISSING,
                severity="critical",
                message=f"Test function '{func_name}' has no assertions",
                location=f"{file_path}:{func_node.lineno}",
                suggestion="Add assert statements or assertion methods to validate test results",
                test_name=func_name
            ))
        
        # Check function complexity (basic check)
        if len(func_node.body) > 50:  # More than 50 statements
            issues.append(ValidationIssue(
                type=ValidationIssueType.QUALITY_ISSUE,
                severity="minor",
                message=f"Test function '{func_name}' is very long",
                location=f"{file_path}:{func_node.lineno}",
                suggestion="Consider splitting into smaller, focused test functions",
                test_name=func_name
            ))
        
        return issues
    
    def _validate_javascript_test_structure(self, content: str, file_path: str) -> List[ValidationIssue]:
        """Basic JavaScript test validation."""
        
        issues = []
        
        # Check for basic test patterns
        test_patterns = [r'test\s*\(', r'it\s*\(', r'describe\s*\(']
        has_tests = any(re.search(pattern, content) for pattern in test_patterns)
        
        if not has_tests:
            issues.append(ValidationIssue(
                type=ValidationIssueType.QUALITY_ISSUE,
                severity="major",
                message=f"No test functions found in {file_path}",
                location=file_path,
                suggestion="Add test(), it(), or describe() blocks",
                test_name=file_path
            ))
        
        # Check for assertions/expectations
        assertion_patterns = [r'expect\s*\(', r'assert\s*\(', r'\.toBe\s*\(', r'\.toEqual\s*\(']
        has_assertions = any(re.search(pattern, content) for pattern in assertion_patterns)
        
        if has_tests and not has_assertions:
            issues.append(ValidationIssue(
                type=ValidationIssueType.ASSERTION_MISSING,
                severity="critical",
                message=f"Tests in {file_path} have no assertions",
                location=file_path,
                suggestion="Add expect() statements or assertions to validate test results",
                test_name=file_path
            ))
        
        return issues
    
    async def _validate_dependencies(self, test_env: TestEnvironment) -> List[ValidationIssue]:
        """Validate test dependencies and imports."""
        
        issues = []
        
        # Change to test directory for installation
        original_cwd = os.getcwd()
        os.chdir(test_env.temp_directory)
        
        try:
            # Install required packages
            if test_env.required_packages and test_env.framework in ['pytest', 'unittest']:
                try:
                    logger.debug("Installing Python test dependencies...")
                    result = await asyncio.create_subprocess_exec(
                        *["pip", "install", "-r", "requirements.txt"],
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    stdout, stderr = await result.communicate()
                    
                    if result.returncode != 0:
                        issues.append(ValidationIssue(
                            type=ValidationIssueType.IMPORT_ERROR,
                            severity="critical",
                            message=f"Failed to install dependencies: {stderr.decode()}",
                            suggestion="Check package requirements and availability",
                            location="requirements.txt"
                        ))
                    else:
                        logger.debug("Dependencies installed successfully")
                        
                except Exception as e:
                    issues.append(ValidationIssue(
                        type=ValidationIssueType.IMPORT_ERROR,
                        severity="critical",
                        message=f"Dependency installation failed: {str(e)}",
                        suggestion="Check pip availability and package names"
                    ))
            
            # For Jest, check if npm/node is available
            elif test_env.framework == "jest":
                try:
                    result = await asyncio.create_subprocess_exec(
                        *["node", "--version"],
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    await result.communicate()
                    
                    if result.returncode != 0:
                        issues.append(ValidationIssue(
                            type=ValidationIssueType.IMPORT_ERROR,
                            severity="critical",
                            message="Node.js not available for Jest tests",
                            suggestion="Install Node.js and npm to run Jest tests"
                        ))
                except Exception as e:
                    issues.append(ValidationIssue(
                        type=ValidationIssueType.IMPORT_ERROR,
                        severity="critical",
                        message=f"Node.js check failed: {str(e)}",
                        suggestion="Install Node.js runtime for JavaScript tests"
                    ))
        
        finally:
            os.chdir(original_cwd)
        
        return issues
    
    async def _execute_tests(self, test_env: TestEnvironment) -> List[TestResult]:
        """Execute all tests and collect results."""
        
        test_results = []
        original_cwd = os.getcwd()
        
        try:
            os.chdir(test_env.temp_directory)
            
            if test_env.framework == "pytest":
                test_results = await self._execute_pytest(test_env)
            elif test_env.framework == "jest":
                test_results = await self._execute_jest(test_env)
            elif test_env.framework == "unittest":
                test_results = await self._execute_unittest(test_env)
            else:
                logger.warning(f"Unsupported framework for execution: {test_env.framework}")
        
        except Exception as e:
            logger.error(f"Test execution failed: {e}")
            # Create error result for failed execution
            test_results = [TestResult(
                test_id="execution_error",
                test_name="Test Execution",
                status=TestStatus.ERROR,
                execution_time=0.0,
                error_message=f"Test execution failed: {str(e)}",
                framework=test_env.framework
            )]
        
        finally:
            os.chdir(original_cwd)
        
        return test_results
    
    async def _execute_pytest(self, test_env: TestEnvironment) -> List[TestResult]:
        """Execute pytest and parse results."""
        
        pytest_args = test_env.get_pytest_args()
        cmd = ["python", "-m", "pytest"] + pytest_args + ["tests/"]
        
        logger.debug(f"Executing pytest: {' '.join(cmd)}")
        
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=dict(os.environ, **test_env.environment_variables)
            )
            
            # Wait for process with timeout
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=test_env.timeout_seconds
                )
            except asyncio.TimeoutError:
                process.kill()
                await process.wait()
                
                return [TestResult(
                    test_id="timeout_error",
                    test_name="Test Execution Timeout",
                    status=TestStatus.TIMEOUT,
                    execution_time=test_env.timeout_seconds,
                    error_message=f"Test execution timed out after {test_env.timeout_seconds}s",
                    framework="pytest"
                )]
            
            # Parse JSON results if available
            json_file = Path(test_env.temp_directory) / "test_results.json"
            if json_file.exists():
                return self._parse_pytest_json_results(json_file)
            else:
                return self._parse_pytest_text_results(stdout.decode(), stderr.decode())
        
        except Exception as e:
            logger.error(f"Pytest execution failed: {e}")
            return [TestResult(
                test_id="pytest_error",
                test_name="Pytest Execution Error",
                status=TestStatus.ERROR,
                execution_time=0.0,
                error_message=str(e),
                framework="pytest"
            )]
    
    def _parse_pytest_json_results(self, json_file: Path) -> List[TestResult]:
        """Parse pytest JSON report."""
        
        results = []
        
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            for test in data.get("tests", []):
                status_map = {
                    "passed": TestStatus.PASSED,
                    "failed": TestStatus.FAILED,
                    "error": TestStatus.ERROR,
                    "skipped": TestStatus.SKIPPED
                }
                
                result = TestResult(
                    test_id=test.get("nodeid", "unknown"),
                    test_name=test.get("nodeid", "unknown").split("::")[-1],
                    status=status_map.get(test.get("outcome"), TestStatus.ERROR),
                    execution_time=test.get("duration", 0.0),
                    output=test.get("call", {}).get("stdout", "") if test.get("call") else "",
                    error_message=self._extract_error_message(test) if test.get("outcome") in ["failed", "error"] else None,
                    traceback=self._extract_traceback(test) if test.get("outcome") in ["failed", "error"] else None,
                    framework="pytest",
                    assertions_count=self._count_assertions_in_output(
                        test.get("call", {}).get("stdout", "") if test.get("call") else ""
                    )
                )
                results.append(result)
                
        except Exception as e:
            logger.error(f"Failed to parse pytest JSON results: {e}")
            results = [TestResult(
                test_id="parse_error",
                test_name="JSON Parse Error",
                status=TestStatus.ERROR,
                execution_time=0.0,
                error_message=f"Failed to parse results: {str(e)}",
                framework="pytest"
            )]
        
        return results
    
    def _parse_pytest_text_results(self, stdout: str, stderr: str) -> List[TestResult]:
        """Parse pytest text output as fallback."""
        
        # This is a simplified implementation
        # In production, you'd want more sophisticated parsing
        
        results = []
        
        # Look for test results in output
        test_pattern = re.compile(r'test_\w+.*?(PASSED|FAILED|ERROR|SKIPPED)')
        matches = test_pattern.findall(stdout + stderr)
        
        for i, match in enumerate(matches):
            status_map = {
                "PASSED": TestStatus.PASSED,
                "FAILED": TestStatus.FAILED,
                "ERROR": TestStatus.ERROR,
                "SKIPPED": TestStatus.SKIPPED
            }
            
            result = TestResult(
                test_id=f"test_{i}",
                test_name=f"test_{i}",
                status=status_map.get(match, TestStatus.ERROR),
                execution_time=1.0,  # Default since we can't parse from text
                framework="pytest"
            )
            results.append(result)
        
        # If no tests found, create a default error
        if not results:
            results = [TestResult(
                test_id="no_tests",
                test_name="No Tests Found",
                status=TestStatus.ERROR,
                execution_time=0.0,
                error_message="No test results could be parsed from output",
                framework="pytest"
            )]
        
        return results
    
    async def _execute_jest(self, test_env: TestEnvironment) -> List[TestResult]:
        """Execute Jest tests."""
        
        # This is a placeholder implementation
        # Full Jest support would require Node.js setup
        
        logger.warning("Jest execution not fully implemented")
        
        return [TestResult(
            test_id="jest_placeholder",
            test_name="Jest Test (Not Implemented)",
            status=TestStatus.SKIPPED,
            execution_time=0.0,
            error_message="Jest execution not implemented",
            framework="jest"
        )]
    
    async def _execute_unittest(self, test_env: TestEnvironment) -> List[TestResult]:
        """Execute unittest tests."""
        
        cmd = ["python", "-m", "unittest", "discover", "-s", "tests", "-v"]
        
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await asyncio.wait_for(
                process.communicate(),
                timeout=test_env.timeout_seconds
            )
            
            return self._parse_unittest_results(stdout.decode(), stderr.decode())
            
        except asyncio.TimeoutError:
            return [TestResult(
                test_id="timeout",
                test_name="Unittest Timeout",
                status=TestStatus.TIMEOUT,
                execution_time=test_env.timeout_seconds,
                error_message="Test execution timed out",
                framework="unittest"
            )]
        except Exception as e:
            return [TestResult(
                test_id="error",
                test_name="Unittest Error",
                status=TestStatus.ERROR,
                execution_time=0.0,
                error_message=str(e),
                framework="unittest"
            )]
    
    def _parse_unittest_results(self, stdout: str, stderr: str) -> List[TestResult]:
        """Parse unittest output."""
        
        results = []
        
        # Basic unittest output parsing
        # This is simplified - real implementation would be more robust
        
        test_pattern = re.compile(r'test_\w+.*?(ok|FAIL|ERROR)')
        matches = test_pattern.findall(stdout + stderr)
        
        for i, match in enumerate(matches):
            status_map = {
                "ok": TestStatus.PASSED,
                "FAIL": TestStatus.FAILED,
                "ERROR": TestStatus.ERROR
            }
            
            result = TestResult(
                test_id=f"unittest_{i}",
                test_name=f"unittest_{i}",
                status=status_map.get(match, TestStatus.ERROR),
                execution_time=1.0,
                framework="unittest"
            )
            results.append(result)
        
        return results if results else [TestResult(
            test_id="no_tests",
            test_name="No Tests",
            status=TestStatus.ERROR,
            execution_time=0.0,
            error_message="No unittest results found",
            framework="unittest"
        )]
    
    def _extract_error_message(self, test_data: Dict[str, Any]) -> Optional[str]:
        """Extract error message from test result."""
        
        if "call" in test_data and test_data["call"]:
            return test_data["call"].get("longrepr", "")
        
        return None
    
    def _extract_traceback(self, test_data: Dict[str, Any]) -> Optional[str]:
        """Extract traceback from test result."""
        
        if "call" in test_data and test_data["call"]:
            return test_data["call"].get("longrepr", "")
        
        return None
    
    def _count_assertions_in_output(self, output: str) -> int:
        """Count assertions in test output."""
        
        if not output:
            return 0
        
        # Count various assertion patterns
        patterns = [
            r'assert\s+',
            r'assertEqual',
            r'assertTrue',
            r'assertFalse',
            r'expect\(',
            r'\.toBe\(',
            r'\.toEqual\('
        ]
        
        count = 0
        for pattern in patterns:
            count += len(re.findall(pattern, output, re.IGNORECASE))
        
        return count
    
    async def _analyze_coverage(self, test_env: TestEnvironment) -> Dict[str, float]:
        """Analyze test coverage."""
        
        coverage_data = {
            'overall': 0.0,
            'statement': 0.0,
            'branch': 0.0,
            'function': 0.0
        }
        
        try:
            # Look for coverage JSON file
            coverage_file = Path(test_env.temp_directory) / "coverage.json"
            
            if coverage_file.exists():
                with open(coverage_file, 'r') as f:
                    data = json.load(f)
                
                # Parse coverage data (structure varies by tool)
                totals = data.get("totals", {})
                coverage_data['overall'] = totals.get("percent_covered", 0.0)
                coverage_data['statement'] = totals.get("percent_covered_display", 0.0)
                
            else:
                logger.debug("No coverage file found, using default values")
                
        except Exception as e:
            logger.warning(f"Failed to analyze coverage: {e}")
        
        return coverage_data
    
    async def _validate_test_effectiveness(
        self,
        test_files: List[Dict[str, Any]],
        context: Optional[Dict[str, Any]]
    ) -> List[ValidationIssue]:
        """Validate that tests are effective at preventing disasters."""
        
        issues = []
        
        # Check for PR #23 protection patterns
        pr23_protection = self._check_disaster_prevention_patterns(test_files)
        
        if pr23_protection['file_integrity_score'] < 20:
            issues.append(ValidationIssue(
                type=ValidationIssueType.DISASTER_PREVENTION,
                severity="critical",
                message="Tests lack file integrity checks (PR #23 protection)",
                suggestion="Add tests that verify files exist and have expected content length",
                location="test suite"
            ))
        
        if pr23_protection['content_preservation_score'] < 15:
            issues.append(ValidationIssue(
                type=ValidationIssueType.DISASTER_PREVENTION,
                severity="major",
                message="Tests don't validate content preservation",
                suggestion="Add tests that verify only intended content is modified",
                location="test suite"
            ))
        
        # Check for meaningful assertions
        total_assertions = sum(
            self._count_assertions_in_content(test_file.get('content', ''))
            for test_file in test_files
        )
        
        if total_assertions < 3:
            issues.append(ValidationIssue(
                type=ValidationIssueType.ASSERTION_MISSING,
                severity="major",
                message="Test suite has insufficient assertions",
                suggestion="Add more meaningful assertions to validate business logic",
                location="test suite"
            ))
        
        # Check for edge case coverage
        edge_case_coverage = self._check_edge_case_patterns(test_files)
        
        if not edge_case_coverage:
            issues.append(ValidationIssue(
                type=ValidationIssueType.COVERAGE_GAP,
                severity="minor",
                message="Tests don't cover edge cases",
                suggestion="Add tests for empty inputs, boundary values, and error conditions",
                location="test suite"
            ))
        
        return issues
    
    def _check_disaster_prevention_patterns(self, test_files: List[Dict[str, Any]]) -> Dict[str, float]:
        """Check for disaster prevention patterns in tests."""
        
        scores = {
            'file_integrity_score': 0.0,
            'content_preservation_score': 0.0,
            'edge_case_score': 0.0,
            'error_handling_score': 0.0
        }
        
        for test_file in test_files:
            content = test_file.get('content', '').lower()
            
            # File integrity patterns
            for pattern in self.disaster_prevention_patterns['file_integrity_checks']:
                if re.search(pattern, content, re.IGNORECASE):
                    scores['file_integrity_score'] += 5.0
            
            # Content preservation patterns
            for pattern in self.disaster_prevention_patterns['content_preservation']:
                if re.search(pattern, content, re.IGNORECASE):
                    scores['content_preservation_score'] += 3.0
            
            # Edge case patterns
            for pattern in self.disaster_prevention_patterns['edge_case_protection']:
                if re.search(pattern, content, re.IGNORECASE):
                    scores['edge_case_score'] += 2.0
            
            # Error handling patterns
            for pattern in self.disaster_prevention_patterns['error_handling']:
                if re.search(pattern, content, re.IGNORECASE):
                    scores['error_handling_score'] += 2.0
        
        # Cap scores at reasonable maximums
        scores['file_integrity_score'] = min(scores['file_integrity_score'], 30.0)
        scores['content_preservation_score'] = min(scores['content_preservation_score'], 20.0)
        scores['edge_case_score'] = min(scores['edge_case_score'], 15.0)
        scores['error_handling_score'] = min(scores['error_handling_score'], 15.0)
        
        return scores
    
    def _count_assertions_in_content(self, content: str) -> int:
        """Count assertions in test content."""
        
        if not content:
            return 0
        
        assertion_patterns = [
            r'assert\s+',
            r'assertEqual',
            r'assertTrue',
            r'assertFalse',
            r'assertIn',
            r'assertNotIn',
            r'expect\(',
            r'\.toBe\(',
            r'\.toEqual\(',
            r'\.toThrow\('
        ]
        
        count = 0
        for pattern in assertion_patterns:
            count += len(re.findall(pattern, content, re.IGNORECASE))
        
        return count
    
    def _check_edge_case_patterns(self, test_files: List[Dict[str, Any]]) -> bool:
        """Check if tests cover edge cases."""
        
        edge_case_indicators = [
            'empty', 'null', 'none', 'zero', 'negative',
            'boundary', 'limit', 'maximum', 'minimum',
            'invalid', 'error', 'exception', 'edge'
        ]
        
        for test_file in test_files:
            content = test_file.get('content', '').lower()
            
            if any(indicator in content for indicator in edge_case_indicators):
                return True
        
        return False
    
    def _calculate_quality_score(self, result: TestValidationResult) -> float:
        """Calculate overall test quality score (0-100)."""
        
        score = 0.0
        
        # Base score for having tests
        if result.total_tests > 0:
            score += 20.0
        
        # Success rate contribution (0-30 points)
        if result.total_tests > 0:
            success_rate = result.get_success_rate() / 100.0
            score += success_rate * 30.0
        
        # Coverage contribution (0-20 points)
        coverage = result.overall_coverage_percentage / 100.0
        score += coverage * 20.0
        
        # Issue penalty (-1 to -5 points per issue based on severity)
        for issue in result.validation_issues:
            if issue.severity == "critical":
                score -= 5.0
            elif issue.severity == "major":
                score -= 3.0
            else:  # minor
                score -= 1.0
        
        # Assertion quality bonus (0-15 points)
        total_assertions = sum(r.assertions_count for r in result.test_results)
        if total_assertions >= 10:
            score += 15.0
        elif total_assertions >= 5:
            score += 10.0
        elif total_assertions >= 3:
            score += 5.0
        
        # Performance bonus/penalty (0-15 points)
        if result.average_test_time <= 1.0:
            score += 15.0
        elif result.average_test_time <= 5.0:
            score += 10.0
        elif result.average_test_time > 10.0:
            score -= 5.0
        
        return max(0.0, min(100.0, score))
    
    def _calculate_disaster_prevention_score(
        self,
        test_files: List[Dict[str, Any]],
        result: TestValidationResult
    ) -> float:
        """Calculate disaster prevention score (0-100)."""
        
        # Get pattern analysis
        patterns = self._check_disaster_prevention_patterns(test_files)
        
        # Calculate weighted score
        score = (
            patterns['file_integrity_score'] * 1.0 +      # Up to 30 points
            patterns['content_preservation_score'] * 1.0 + # Up to 20 points  
            patterns['edge_case_score'] * 1.0 +           # Up to 15 points
            patterns['error_handling_score'] * 1.0        # Up to 15 points
        )
        
        # Bonus for test categories that prevent disasters
        for test_result in result.test_results:
            if test_result.category == "edge_case":
                score += 5.0
            elif test_result.category == "error_handling":
                score += 3.0
        
        # Penalty for critical validation issues
        critical_issues = result.get_critical_issues()
        score -= len(critical_issues) * 10.0
        
        # Bonus for high test success rate (stable tests prevent disasters)
        if result.total_tests > 0:
            success_rate = result.get_success_rate() / 100.0
            score += success_rate * 20.0
        
        return max(0.0, min(100.0, score))
    
    def _generate_recommendations(self, result: TestValidationResult) -> List[str]:
        """Generate recommendations for improving test quality."""
        
        recommendations = []
        
        # Test execution recommendations
        if result.failed_tests > 0:
            recommendations.append(f"Fix {result.failed_tests} failing tests before deployment")
        
        if result.error_tests > 0:
            recommendations.append(f"Resolve {result.error_tests} test execution errors")
        
        # Performance recommendations
        if result.average_test_time > 10.0:
            recommendations.append("Optimize slow-running tests to improve feedback speed")
        
        # Coverage recommendations
        if result.overall_coverage_percentage < 70.0:
            recommendations.append("Increase test coverage to at least 70% for better confidence")
        
        # Quality recommendations based on issues
        critical_issues = result.get_critical_issues()
        if critical_issues:
            recommendations.append(f"Address {len(critical_issues)} critical issues before approval")
        
        # Disaster prevention recommendations
        if result.disaster_prevention_score < 50.0:
            recommendations.append("Add more tests that prevent disasters like PR #23 (file integrity, content preservation)")
        
        # Assertion recommendations
        total_assertions = sum(r.assertions_count for r in result.test_results)
        if result.total_tests > 0 and total_assertions / result.total_tests < 2:
            recommendations.append("Add more meaningful assertions to each test")
        
        # Framework-specific recommendations
        if result.framework_detected == "jest" and any("not implemented" in r.error_message or "" for r in result.test_results):
            recommendations.append("Complete Jest test execution implementation")
        
        return recommendations
    
    def _determine_overall_success(self, result: TestValidationResult) -> bool:
        """Determine if test validation was successful overall."""
        
        # Must have tests
        if result.total_tests == 0:
            return False
        
        # No critical issues allowed
        if result.has_critical_issues():
            return False
        
        # Must have reasonable success rate
        success_rate = result.get_success_rate()
        if success_rate < 80.0 and self.strict_validation:
            return False
        elif success_rate < 50.0:  # Even lenient mode has minimum
            return False
        
        # Quality and disaster prevention thresholds
        if self.strict_validation:
            return (result.quality_score >= 70.0 and 
                    result.disaster_prevention_score >= 40.0)
        else:
            return (result.quality_score >= 50.0 and 
                    result.disaster_prevention_score >= 25.0)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get validator performance metrics."""
        
        return {
            'validator_config': {
                'default_timeout': self.default_timeout,
                'max_execution_time': self.max_execution_time,
                'enable_coverage': self.enable_coverage,
                'strict_validation': self.strict_validation
            },
            'supported_frameworks': list(self.framework_configs.keys()),
            'metrics': self.metrics.get_metrics_summary()
        }
    
    def __str__(self) -> str:
        return f"TestValidator(timeout={self.default_timeout}s, coverage={self.enable_coverage})"