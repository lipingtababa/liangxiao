# Story 6.2: Test Validation System

## Story Details
- **ID**: 6.2
- **Title**: Implement Test Validation and Execution System
- **Milestone**: Milestone 6 - Tester Agent
- **Points**: 5
- **Priority**: P1 (Essential)
- **Dependencies**: Story 6.1 (Tester Agent), Story 4.2 (Task Pair System)

## Description

### Overview
Create a robust test validation system that executes tests generated by the Tester Agent, validates their correctness, and ensures they catch the types of disasters that the system is designed to prevent (like PR #23).

### Why This Is Important
- Validates that generated tests actually work and catch bugs
- Ensures test quality before they're included in PRs
- Prevents regression by validating existing functionality
- Provides confidence that the system improvements are real
- Creates feedback loop for test generation improvement

### Context
The Tester Agent generates comprehensive tests, but we need to ensure those tests are executable, meaningful, and actually catch the problems they're designed to prevent. This validation system runs the tests and verifies their effectiveness.

## Acceptance Criteria

### Required
- [ ] Test execution engine that runs generated tests
- [ ] Validation of test syntax and imports
- [ ] Test effectiveness measurement (can catch known issues)
- [ ] Test coverage analysis for generated code
- [ ] Integration with existing test frameworks (pytest, jest, etc.)
- [ ] Test result reporting and analysis
- [ ] Failure analysis when tests don't work as expected
- [ ] Test performance monitoring (execution time)
- [ ] Cleanup of test artifacts after execution
- [ ] Integration with Tester-Navigator pair workflow

## Technical Details

### Test Validation Engine
```python
# agents/tester/validator.py
from typing import List, Dict, Any, Optional
import subprocess
import pytest
import tempfile
import os
from pathlib import Path
import logging
from enum import Enum

logger = logging.getLogger(__name__)

class TestStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    ERROR = "error"
    SKIPPED = "skipped"

class TestResult(BaseModel):
    """Result of test execution."""
    test_id: str
    status: TestStatus
    execution_time: float
    output: str = ""
    error_message: Optional[str] = None
    coverage_percentage: Optional[float] = None
    assertions_count: int = 0

class TestValidationResult(BaseModel):
    """Complete validation result."""
    overall_success: bool
    total_tests: int
    passed_tests: int
    failed_tests: int
    error_tests: int
    execution_time: float
    coverage_percentage: float
    test_results: List[TestResult]
    validation_issues: List[str] = []
    recommendations: List[str] = []

class TestValidator:
    """
    Validates and executes tests generated by Tester Agent.
    
    Ensures tests are syntactically correct, executable, and effective
    at catching the issues they're designed to prevent.
    """
    
    def __init__(self):
        self.temp_dir = None
        
    async def validate_test_suite(
        self,
        test_artifacts: List[Dict[str, Any]],
        target_code: Optional[str] = None,
        context: Dict[str, Any] = None
    ) -> TestValidationResult:
        """
        Validate complete test suite.
        
        Args:
            test_artifacts: Test files generated by Tester Agent
            target_code: Code being tested (for coverage analysis)
            context: Additional context for validation
            
        Returns:
            Complete validation result
        """
        logger.info(f"Validating test suite with {len(test_artifacts)} test files")
        
        try:
            # Create temporary test environment
            with tempfile.TemporaryDirectory() as temp_dir:
                self.temp_dir = temp_dir
                
                # Setup test environment
                await self._setup_test_environment(test_artifacts, target_code)
                
                # Validate test syntax
                syntax_issues = await self._validate_test_syntax(test_artifacts)
                
                # Execute tests
                test_results = await self._execute_tests(test_artifacts)
                
                # Analyze coverage
                coverage_percentage = await self._analyze_coverage()
                
                # Validate test effectiveness
                effectiveness_issues = await self._validate_test_effectiveness(
                    test_artifacts, 
                    context
                )
                
                # Generate recommendations
                recommendations = await self._generate_recommendations(
                    test_results,
                    syntax_issues + effectiveness_issues
                )
                
                return TestValidationResult(
                    overall_success=len(syntax_issues + effectiveness_issues) == 0,
                    total_tests=len(test_results),
                    passed_tests=len([r for r in test_results if r.status == TestStatus.PASSED]),
                    failed_tests=len([r for r in test_results if r.status == TestStatus.FAILED]),
                    error_tests=len([r for r in test_results if r.status == TestStatus.ERROR]),
                    execution_time=sum(r.execution_time for r in test_results),
                    coverage_percentage=coverage_percentage,
                    test_results=test_results,
                    validation_issues=syntax_issues + effectiveness_issues,
                    recommendations=recommendations
                )
                
        except Exception as e:
            logger.error(f"Test validation failed: {e}")
            return TestValidationResult(
                overall_success=False,
                total_tests=0,
                passed_tests=0,
                failed_tests=0,
                error_tests=0,
                execution_time=0.0,
                coverage_percentage=0.0,
                test_results=[],
                validation_issues=[f"Validation system error: {str(e)}"],
                recommendations=["Fix validation system issues before proceeding"]
            )
    
    async def _setup_test_environment(
        self, 
        test_artifacts: List[Dict[str, Any]], 
        target_code: Optional[str]
    ):
        """Setup temporary test environment."""
        
        # Create test directory structure
        test_dir = Path(self.temp_dir) / "tests"
        test_dir.mkdir()
        
        src_dir = Path(self.temp_dir) / "src"
        src_dir.mkdir()
        
        # Write test files
        for artifact in test_artifacts:
            if artifact.get("type") == "test":
                test_path = test_dir / artifact["path"]
                test_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(test_path, "w") as f:
                    f.write(artifact["content"])
        
        # Write target code if provided
        if target_code:
            src_path = src_dir / "main.py"  # Simplified for validation
            with open(src_path, "w") as f:
                f.write(target_code)
        
        # Create basic setup files
        self._create_pytest_config()
        self._create_requirements_file()
    
    def _create_pytest_config(self):
        """Create pytest configuration."""
        config = """
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_functions = test_*
addopts = --verbose --tb=short --cov=src --cov-report=term --cov-report=json
"""
        config_path = Path(self.temp_dir) / "pytest.ini"
        with open(config_path, "w") as f:
            f.write(config)
    
    def _create_requirements_file(self):
        """Create basic requirements for testing."""
        requirements = """
pytest>=7.0.0
pytest-cov>=4.0.0
pytest-mock>=3.0.0
"""
        req_path = Path(self.temp_dir) / "requirements.txt"
        with open(req_path, "w") as f:
            f.write(requirements)
    
    async def _validate_test_syntax(
        self, 
        test_artifacts: List[Dict[str, Any]]
    ) -> List[str]:
        """Validate test files have correct syntax."""
        
        issues = []
        
        for artifact in test_artifacts:
            if artifact.get("type") == "test":
                try:
                    # Attempt to compile the test code
                    compile(artifact["content"], artifact["path"], "exec")
                    logger.debug(f"Syntax valid for {artifact['path']}")
                    
                except SyntaxError as e:
                    issue = f"Syntax error in {artifact['path']}: {e.msg} (line {e.lineno})"
                    issues.append(issue)
                    logger.error(issue)
                    
                except Exception as e:
                    issue = f"Compilation error in {artifact['path']}: {str(e)}"
                    issues.append(issue)
                    logger.error(issue)
        
        return issues
    
    async def _execute_tests(
        self, 
        test_artifacts: List[Dict[str, Any]]
    ) -> List[TestResult]:
        """Execute all tests and collect results."""
        
        test_results = []
        
        # Change to test directory
        original_cwd = os.getcwd()
        os.chdir(self.temp_dir)
        
        try:
            # Install requirements
            subprocess.run([
                "pip", "install", "-r", "requirements.txt"
            ], capture_output=True, text=True, check=True)
            
            # Run pytest with JSON output
            result = subprocess.run([
                "python", "-m", "pytest", 
                "tests/", 
                "--json-report",
                "--json-report-file=test_results.json"
            ], capture_output=True, text=True)
            
            # Parse results
            test_results = self._parse_pytest_results(result)
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Test execution failed: {e}")
            # Create error results for each test file
            for artifact in test_artifacts:
                if artifact.get("type") == "test":
                    test_results.append(TestResult(
                        test_id=artifact["path"],
                        status=TestStatus.ERROR,
                        execution_time=0.0,
                        error_message=f"Test execution failed: {e}",
                        assertions_count=0
                    ))
        
        finally:
            os.chdir(original_cwd)
        
        return test_results
    
    def _parse_pytest_results(self, pytest_result) -> List[TestResult]:
        """Parse pytest results into TestResult objects."""
        
        results = []
        
        try:
            # Try to load JSON report if available
            json_path = Path(self.temp_dir) / "test_results.json"
            if json_path.exists():
                with open(json_path) as f:
                    data = json.load(f)
                
                for test in data.get("tests", []):
                    results.append(TestResult(
                        test_id=test["nodeid"],
                        status=TestStatus(test["outcome"]),
                        execution_time=test.get("duration", 0.0),
                        output=test.get("call", {}).get("longrepr", ""),
                        error_message=test.get("call", {}).get("longrepr") if test["outcome"] in ["failed", "error"] else None,
                        assertions_count=self._count_assertions_in_output(test.get("call", {}).get("longrepr", ""))
                    ))
                    
        except Exception as e:
            logger.warning(f"Could not parse JSON results: {e}, falling back to text parsing")
            # Fallback to parsing text output
            results = self._parse_text_results(pytest_result.stdout, pytest_result.stderr)
        
        return results
    
    def _count_assertions_in_output(self, output: str) -> int:
        """Count assertion statements in test output."""
        if not output:
            return 0
        return output.count("assert ") + output.count("assertEqual") + output.count("assertTrue")
    
    async def _analyze_coverage(self) -> float:
        """Analyze test coverage percentage."""
        
        try:
            coverage_path = Path(self.temp_dir) / "coverage.json"
            if coverage_path.exists():
                with open(coverage_path) as f:
                    coverage_data = json.load(f)
                    return coverage_data.get("totals", {}).get("percent_covered", 0.0)
        except Exception as e:
            logger.warning(f"Could not analyze coverage: {e}")
        
        return 0.0
    
    async def _validate_test_effectiveness(
        self,
        test_artifacts: List[Dict[str, Any]],
        context: Dict[str, Any]
    ) -> List[str]:
        """Validate that tests are effective at catching issues."""
        
        issues = []
        
        # Check if tests would catch PR #23 type disasters
        pr23_protection = self._check_pr23_protection(test_artifacts)
        if not pr23_protection:
            issues.append("Tests do not protect against PR #23 type disasters (wholesale file deletion)")
        
        # Check for meaningful assertions
        meaningful_tests = self._check_meaningful_assertions(test_artifacts)
        if not meaningful_tests:
            issues.append("Tests lack meaningful assertions that validate business logic")
        
        # Check for edge case coverage
        edge_case_coverage = self._check_edge_case_coverage(test_artifacts)
        if not edge_case_coverage:
            issues.append("Tests do not cover important edge cases")
        
        return issues
    
    def _check_pr23_protection(self, test_artifacts: List[Dict[str, Any]]) -> bool:
        """Check if tests protect against PR #23 disasters."""
        
        for artifact in test_artifacts:
            content = artifact.get("content", "")
            
            # Look for tests that validate targeted changes
            if any(keyword in content.lower() for keyword in [
                "preserve", "only remove", "targeted", "specific phrase", 
                "other content unchanged", "file structure maintained"
            ]):
                return True
        
        return False
    
    def _check_meaningful_assertions(self, test_artifacts: List[Dict[str, Any]]) -> bool:
        """Check for meaningful business logic assertions."""
        
        meaningful_count = 0
        
        for artifact in test_artifacts:
            content = artifact.get("content", "")
            
            # Count various assertion types
            assertion_patterns = [
                "assert_equal", "assert_true", "assert_false",
                "assert_in", "assert_not_in", "assert_contains",
                "assertEqual", "assertTrue", "assertFalse",
                "expect(", ".toBe(", ".toEqual("
            ]
            
            for pattern in assertion_patterns:
                meaningful_count += content.count(pattern)
        
        return meaningful_count >= 3  # At least 3 meaningful assertions
    
    def _check_edge_case_coverage(self, test_artifacts: List[Dict[str, Any]]) -> bool:
        """Check for edge case test coverage."""
        
        for artifact in test_artifacts:
            content = artifact.get("content", "")
            
            # Look for edge case indicators
            edge_case_patterns = [
                "empty", "null", "none", "zero", "negative",
                "boundary", "limit", "maximum", "minimum",
                "invalid", "error", "exception", "edge"
            ]
            
            if any(pattern in content.lower() for pattern in edge_case_patterns):
                return True
        
        return False
    
    async def _generate_recommendations(
        self,
        test_results: List[TestResult],
        validation_issues: List[str]
    ) -> List[str]:
        """Generate recommendations for improving tests."""
        
        recommendations = []
        
        # Analyze test results for recommendations
        failed_tests = [r for r in test_results if r.status == TestStatus.FAILED]
        error_tests = [r for r in test_results if r.status == TestStatus.ERROR]
        
        if failed_tests:
            recommendations.append(f"Fix {len(failed_tests)} failing tests before proceeding")
        
        if error_tests:
            recommendations.append(f"Resolve {len(error_tests)} test errors (likely import/setup issues)")
        
        # Performance recommendations
        slow_tests = [r for r in test_results if r.execution_time > 5.0]
        if slow_tests:
            recommendations.append(f"Optimize {len(slow_tests)} slow-running tests")
        
        # Coverage recommendations
        total_assertions = sum(r.assertions_count for r in test_results)
        if total_assertions < 10:
            recommendations.append("Add more meaningful assertions to increase test thoroughness")
        
        # Issue-based recommendations
        if validation_issues:
            recommendations.append("Address validation issues before test approval")
        
        return recommendations
```

### Integration with Tester-Navigator Pair
```python
# agents/pairs/tester_navigator_pair.py
from agents.tester.agent import TesterAgent
from agents.navigator.test_reviewer import TestNavigator
from agents.tester.validator import TestValidator
from agents.pairs.task_pair import TaskPair

class TesterNavigatorPair(TaskPair):
    """Specialized pair for test generation and validation."""
    
    def __init__(self):
        tester = TesterAgent()
        navigator = TestNavigator()
        super().__init__(
            tasker_agent=tester,
            navigator_agent=navigator,
            max_iterations=3,
            require_approval=True
        )
        self.validator = TestValidator()
    
    async def execute_test_generation(
        self,
        task: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute test generation with validation."""
        
        logger.info(f"Starting test generation for task {task['id']}")
        
        # Generate tests through pair programming
        result = await self.execute_task(task, context)
        
        if result.success:
            # Validate generated tests
            validation_result = await self.validator.validate_test_suite(
                test_artifacts=result.final_output.get("artifacts", []),
                target_code=context.get("target_code"),
                context=context
            )
            
            # Add validation results to output
            final_output = result.final_output
            final_output["validation"] = validation_result.model_dump()
            final_output["test_summary"] = self._create_test_summary(
                result, 
                validation_result
            )
            
            # Only approve if validation passes
            if not validation_result.overall_success:
                logger.warning("Generated tests failed validation")
                final_output["success"] = False
                final_output["validation_required"] = True
            
            return final_output
        else:
            logger.error(f"Test generation failed: {result.failure_reason}")
            return {
                "success": False,
                "error": result.failure_reason,
                "artifacts": []
            }
    
    def _create_test_summary(
        self, 
        pair_result,
        validation_result: TestValidationResult
    ) -> str:
        """Create summary of test generation and validation."""
        
        iterations = len(pair_result.iterations)
        
        return f"""
## Test Generation Summary

- **Iterations**: {iterations}
- **Tests Generated**: {validation_result.total_tests}
- **Validation Status**: {"✅ PASSED" if validation_result.overall_success else "❌ FAILED"}
- **Test Results**: {validation_result.passed_tests} passed, {validation_result.failed_tests} failed, {validation_result.error_tests} errors
- **Coverage**: {validation_result.coverage_percentage:.1f}%
- **Execution Time**: {validation_result.execution_time:.2f}s

### Validation Issues
{chr(10).join(f"- {issue}" for issue in validation_result.validation_issues) if validation_result.validation_issues else "- None"}

### Recommendations
{chr(10).join(f"- {rec}" for rec in validation_result.recommendations) if validation_result.recommendations else "- None"}

Tests have been thoroughly generated and validated for deployment.
"""
```

## Testing Requirements

### Validator Tests
```python
# tests/test_test_validator.py
import pytest
from agents.tester.validator import TestValidator, TestStatus

@pytest.mark.asyncio
async def test_syntax_validation():
    """Test validation catches syntax errors."""
    validator = TestValidator()
    
    test_artifacts = [{
        "type": "test",
        "path": "test_bad_syntax.py",
        "content": "def test_invalid():\n    assert True  # Missing colon causes error"
    }]
    
    result = await validator.validate_test_suite(test_artifacts)
    
    # Should catch syntax issues
    assert not result.overall_success
    assert len(result.validation_issues) > 0
    assert any("syntax" in issue.lower() for issue in result.validation_issues)

@pytest.mark.asyncio
async def test_pr23_protection_detection():
    """Test detection of PR #23 protection tests."""
    validator = TestValidator()
    
    good_test = [{
        "type": "test",
        "path": "test_targeted_change.py",
        "content": '''
def test_removes_only_target_phrase():
    """Test that only specific phrase is removed, preserving other content."""
    original = "Keep this. Remove 解释文化细节. Keep this too."
    expected = "Keep this. . Keep this too."
    result = remove_phrase(original, "解释文化细节")
    assert result == expected
    assert "Keep this" in result  # Verify preservation
'''
    }]
    
    result = await validator.validate_test_suite(good_test)
    
    # Should recognize PR #23 protection
    pr23_issues = [issue for issue in result.validation_issues 
                   if "PR #23" in issue or "wholesale file deletion" in issue]
    assert len(pr23_issues) == 0  # Should NOT flag this as an issue

@pytest.mark.asyncio
async def test_effectiveness_validation():
    """Test that validator checks test effectiveness."""
    validator = TestValidator()
    
    weak_test = [{
        "type": "test", 
        "path": "test_weak.py",
        "content": "def test_something(): pass"  # No assertions!
    }]
    
    result = await validator.validate_test_suite(weak_test)
    
    # Should flag weak tests
    assert not result.overall_success
    assert any("meaningful assertions" in issue for issue in result.validation_issues)
```

## Dependencies & Risks

### Prerequisites
- Test frameworks (pytest, jest) installed
- Code coverage tools available
- Temporary directory access for test execution
- Sufficient permissions to execute tests

### Risks
- **Test execution failures**: Generated tests might not run properly
- **False positives**: Validator might flag good tests as bad
- **Performance impact**: Test validation can be time-consuming
- **Environment issues**: Missing dependencies cause validation failures

### Mitigations
- Robust error handling for test execution
- Conservative validation with clear feedback
- Timeout limits for long-running tests
- Dependency checking before validation

## Definition of Done

1. ✅ Test validator implemented with execution engine
2. ✅ Syntax validation working for all supported languages
3. ✅ Test effectiveness validation (PR #23 protection check)
4. ✅ Coverage analysis integration
5. ✅ Integration with Tester-Navigator pair workflow
6. ✅ Comprehensive error handling and logging
7. ✅ Performance monitoring for test execution
8. ✅ Unit tests passing for validator functionality
9. ✅ Documentation of validation criteria

## Implementation Notes for AI Agents

### DO
- Validate tests thoroughly before approving
- Check for meaningful assertions and business logic
- Ensure tests protect against known disaster patterns
- Provide specific feedback for test improvement
- Monitor test execution performance

### DON'T
- Don't approve tests without running them
- Don't ignore syntax errors in generated tests
- Don't skip effectiveness validation
- Don't allow tests without assertions
- Don't approve tests that don't protect against regressions

### Common Pitfalls to Avoid
1. Generated tests that don't actually run
2. Tests with no meaningful assertions
3. Missing edge case coverage
4. Tests that don't protect against PR #23 disasters
5. Ignoring test performance issues

## Success Example

Preventing future PR #23 through test validation:
```python
# Generated test that validator approves ✅
def test_targeted_phrase_removal():
    """Ensure only specific phrase removed, preserving all other content."""
    original_readme = read_file("README.md")
    
    # Apply the change
    result = remove_phrase(original_readme, "解释文化细节")
    
    # Validate targeted change
    assert "解释文化细节" not in result
    
    # Validate preservation (THIS PREVENTS PR #23!)
    assert len(result) > len(original_readme) - 50  # Didn't delete everything
    assert "# Project Title" in result  # Header preserved
    assert "## Installation" in result  # Sections preserved
    
    # Validate structure maintained
    assert result.count("##") == original_readme.count("##")
```

## Next Story
Once this story is complete, all critical stories for the multi-agent system are documented. Proceed to fill in remaining PM and Navigator stories for completeness.