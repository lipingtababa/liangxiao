#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•ç¿»è¯‘ç®¡é“å„ä¸ªç»„ä»¶çš„æ€§èƒ½è¡¨ç°
"""

import unittest
import time
import tempfile
import os
import sys
import json
import random
import string
from pathlib import Path
from datetime import datetime, timedelta
import tracemalloc
import cProfile
import pstats
import io
from contextlib import contextmanager
from typing import Callable, Any, Dict, List
import threading
import multiprocessing

# æ·»åŠ scriptsç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'scripts'))

from wechat_extractor import clean_text, extract_from_html
from state_manager import ArticleStateManager
from markdown_generator import (
    sanitize_slug,
    generate_tags,
    format_content_to_markdown,
    generate_markdown,
    batch_generate
)


@contextmanager
def timer(name: str = "Operation"):
    """è®¡æ—¶å™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    start = time.perf_counter()
    yield
    end = time.perf_counter()
    print(f"{name} took {end - start:.4f} seconds")


@contextmanager
def memory_tracker():
    """å†…å­˜è·Ÿè¸ªå™¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    tracemalloc.start()
    yield
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    print(f"Current memory: {current / 1024 / 1024:.2f} MB")
    print(f"Peak memory: {peak / 1024 / 1024:.2f} MB")


def profile_function(func: Callable) -> Callable:
    """æ€§èƒ½åˆ†æè£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()
        result = func(*args, **kwargs)
        profiler.disable()

        # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡
        s = io.StringIO()
        ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        ps.print_stats(10)  # æ‰“å°å‰10ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
        print(s.getvalue())

        return result
    return wrapper


class TestTextProcessingPerformance(unittest.TestCase):
    """æ–‡æœ¬å¤„ç†æ€§èƒ½æµ‹è¯•"""

    def setUp(self):
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        self.short_text = "è¿™æ˜¯ä¸€æ®µçŸ­æ–‡æœ¬" * 10
        self.medium_text = "è¿™æ˜¯ä¸­ç­‰é•¿åº¦çš„æ–‡æœ¬å†…å®¹" * 100
        self.long_text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…å«å„ç§ä¸­æ–‡å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ã€‚" * 1000
        self.very_long_text = "è¶…é•¿æ–‡æœ¬å†…å®¹" * 10000

    def test_clean_text_performance(self):
        """æµ‹è¯•æ–‡æœ¬æ¸…ç†æ€§èƒ½"""
        test_cases = [
            ("çŸ­æ–‡æœ¬", self.short_text, 0.001),
            ("ä¸­ç­‰æ–‡æœ¬", self.medium_text, 0.01),
            ("é•¿æ–‡æœ¬", self.long_text, 0.1),
            ("è¶…é•¿æ–‡æœ¬", self.very_long_text, 1.0)
        ]

        for name, text, max_time in test_cases:
            start = time.perf_counter()
            result = clean_text(text)
            elapsed = time.perf_counter() - start

            self.assertIsNotNone(result)
            self.assertLess(elapsed, max_time,
                          f"{name}æ¸…ç†è€—æ—¶{elapsed:.4f}ç§’ï¼Œè¶…è¿‡{max_time}ç§’é™åˆ¶")
            print(f"{name}æ¸…ç†: {elapsed:.4f}ç§’")

    def test_slug_generation_performance(self):
        """æµ‹è¯•slugç”Ÿæˆæ€§èƒ½"""
        titles = [
            "ç®€å•æ ‡é¢˜",
            "è¿™æ˜¯ä¸€ä¸ªåŒ…å«å¾ˆå¤šä¸­æ–‡å­—ç¬¦çš„è¶…é•¿æ ‡é¢˜éœ€è¦å¤„ç†" * 5,
            "Title with English and ä¸­æ–‡ mixed content @#$%",
            "åŒ…å«emojiçš„æ ‡é¢˜ ğŸ˜ŠğŸ‰ğŸ‘ å’Œç‰¹æ®Šå­—ç¬¦ â„¢Â®Â©"
        ]

        for title in titles:
            start = time.perf_counter()
            slug = sanitize_slug(title)
            elapsed = time.perf_counter() - start

            self.assertIsNotNone(slug)
            self.assertLess(elapsed, 0.01,
                          f"Slugç”Ÿæˆè€—æ—¶{elapsed:.4f}ç§’ï¼Œè¶…è¿‡0.01ç§’é™åˆ¶")

    def test_tag_generation_performance(self):
        """æµ‹è¯•æ ‡ç­¾ç”Ÿæˆæ€§èƒ½"""
        contents = [
            self.short_text,
            self.medium_text,
            self.long_text
        ]

        for i, content in enumerate(contents):
            start = time.perf_counter()
            tags = generate_tags(content, f"æ ‡é¢˜{i}")
            elapsed = time.perf_counter() - start

            self.assertIsNotNone(tags)
            self.assertLess(elapsed, 0.1,
                          f"æ ‡ç­¾ç”Ÿæˆè€—æ—¶{elapsed:.4f}ç§’ï¼Œè¶…è¿‡0.1ç§’é™åˆ¶")


class TestHTMLExtractionPerformance(unittest.TestCase):
    """HTMLæå–æ€§èƒ½æµ‹è¯•"""

    def generate_complex_html(self, paragraphs: int, images: int) -> str:
        """ç”Ÿæˆå¤æ‚çš„HTMLå†…å®¹"""
        html = """
        <html>
        <head>
            <title>æ€§èƒ½æµ‹è¯•æ–‡ç« </title>
            <meta property="og:title" content="æ€§èƒ½æµ‹è¯•æ–‡ç« ">
        </head>
        <body>
            <h1 class="rich_media_title">æ€§èƒ½æµ‹è¯•æ–‡ç« æ ‡é¢˜</h1>
            <span class="rich_media_meta rich_media_meta_nickname">æµ‹è¯•ä½œè€…</span>
            <em id="publish_time">2024-01-15</em>
            <div id="js_content">
        """

        for i in range(paragraphs):
            html += f"<p>è¿™æ˜¯ç¬¬{i+1}æ®µå†…å®¹ï¼ŒåŒ…å«ä¸€äº›æ–‡å­—æè¿°ã€‚</p>\n"
            if i % 5 == 0 and i < images * 5:
                html += f'<img data-src="http://example.com/image{i//5}.jpg" alt="å›¾ç‰‡{i//5}">\n'

        html += """
            </div>
        </body>
        </html>
        """
        return html

    def test_html_extraction_scalability(self):
        """æµ‹è¯•HTMLæå–çš„å¯æ‰©å±•æ€§"""
        test_cases = [
            (10, 2, 0.1),     # 10æ®µè½ï¼Œ2å›¾ç‰‡ï¼Œ0.1ç§’é™åˆ¶
            (50, 10, 0.5),    # 50æ®µè½ï¼Œ10å›¾ç‰‡ï¼Œ0.5ç§’é™åˆ¶
            (100, 20, 1.0),   # 100æ®µè½ï¼Œ20å›¾ç‰‡ï¼Œ1ç§’é™åˆ¶
            (500, 50, 5.0),   # 500æ®µè½ï¼Œ50å›¾ç‰‡ï¼Œ5ç§’é™åˆ¶
        ]

        for paragraphs, images, max_time in test_cases:
            html = self.generate_complex_html(paragraphs, images)

            start = time.perf_counter()
            result = extract_from_html(html, save_images=False)
            elapsed = time.perf_counter() - start

            self.assertIsNotNone(result)
            self.assertEqual(len(result['images']), images)
            self.assertLess(elapsed, max_time,
                          f"æå–{paragraphs}æ®µè½è€—æ—¶{elapsed:.4f}ç§’ï¼Œè¶…è¿‡{max_time}ç§’é™åˆ¶")
            print(f"æå–{paragraphs}æ®µè½ï¼Œ{images}å›¾ç‰‡: {elapsed:.4f}ç§’")

    @profile_function
    def test_html_extraction_profile(self):
        """HTMLæå–æ€§èƒ½åˆ†æ"""
        html = self.generate_complex_html(100, 20)
        result = extract_from_html(html, save_images=False)
        self.assertIsNotNone(result)


class TestStateManagementPerformance(unittest.TestCase):
    """çŠ¶æ€ç®¡ç†æ€§èƒ½æµ‹è¯•"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()
        self.state_file = os.path.join(self.temp_dir, 'state.json')

    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        import shutil
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def test_state_manager_scalability(self):
        """æµ‹è¯•çŠ¶æ€ç®¡ç†å™¨çš„å¯æ‰©å±•æ€§"""
        state_manager = ArticleStateManager(self.state_file)

        # æµ‹è¯•ä¸åŒæ•°é‡çš„æ–‡ç« 
        test_cases = [
            (100, 0.5),    # 100ç¯‡æ–‡ç« ï¼Œ0.5ç§’é™åˆ¶
            (500, 2.0),    # 500ç¯‡æ–‡ç« ï¼Œ2ç§’é™åˆ¶
            (1000, 5.0),   # 1000ç¯‡æ–‡ç« ï¼Œ5ç§’é™åˆ¶
        ]

        for count, max_time in test_cases:
            # æ¸…ç©ºçŠ¶æ€
            state_manager.state_data['articles'] = {}

            start = time.perf_counter()
            for i in range(count):
                article_data = {
                    "title": f"æ–‡ç« {i}",
                    "content": {"text": f"å†…å®¹{i}" * 100}
                }
                state_manager.add_article(f"http://example.com/{i}", article_data)
            elapsed = time.perf_counter() - start

            self.assertEqual(len(state_manager.state_data['articles']), count)
            self.assertLess(elapsed, max_time,
                          f"æ·»åŠ {count}ç¯‡æ–‡ç« è€—æ—¶{elapsed:.4f}ç§’ï¼Œè¶…è¿‡{max_time}ç§’é™åˆ¶")
            print(f"æ·»åŠ {count}ç¯‡æ–‡ç« : {elapsed:.4f}ç§’")

    def test_state_manager_query_performance(self):
        """æµ‹è¯•çŠ¶æ€æŸ¥è¯¢æ€§èƒ½"""
        state_manager = ArticleStateManager(self.state_file)

        # å…ˆæ·»åŠ 1000ç¯‡æ–‡ç« 
        for i in range(1000):
            article_data = {
                "title": f"æ–‡ç« {i}",
                "content": {"text": f"å†…å®¹{i}"}
            }
            state_manager.add_article(f"http://example.com/{i}", article_data)

        # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
        urls_to_check = [f"http://example.com/{i}" for i in range(0, 1000, 10)]

        start = time.perf_counter()
        for url in urls_to_check:
            is_processed = state_manager.is_article_processed(url)
            self.assertTrue(is_processed)
        elapsed = time.perf_counter() - start

        self.assertLess(elapsed, 0.1,
                      f"æŸ¥è¯¢100ä¸ªURLè€—æ—¶{elapsed:.4f}ç§’ï¼Œè¶…è¿‡0.1ç§’é™åˆ¶")
        print(f"æŸ¥è¯¢100ä¸ªURL: {elapsed:.4f}ç§’")

    def test_concurrent_state_access(self):
        """æµ‹è¯•å¹¶å‘çŠ¶æ€è®¿é—®æ€§èƒ½"""
        def worker(thread_id: int, url_count: int):
            """å·¥ä½œçº¿ç¨‹"""
            state_manager = ArticleStateManager(self.state_file)
            for i in range(url_count):
                article_data = {
                    "title": f"çº¿ç¨‹{thread_id}-æ–‡ç« {i}",
                    "content": {"text": f"å†…å®¹{i}"}
                }
                url = f"http://example.com/thread{thread_id}/article{i}"
                state_manager.add_article(url, article_data)

        thread_count = 5
        urls_per_thread = 20

        start = time.perf_counter()

        threads = []
        for i in range(thread_count):
            thread = threading.Thread(target=worker, args=(i, urls_per_thread))
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join()

        elapsed = time.perf_counter() - start

        # éªŒè¯æ‰€æœ‰æ–‡ç« éƒ½è¢«æ·»åŠ 
        state_manager = ArticleStateManager(self.state_file)
        total_articles = len(state_manager.state_data['articles'])
        self.assertEqual(total_articles, thread_count * urls_per_thread)

        self.assertLess(elapsed, 5.0,
                      f"å¹¶å‘å¤„ç†{total_articles}ç¯‡æ–‡ç« è€—æ—¶{elapsed:.4f}ç§’ï¼Œè¶…è¿‡5ç§’é™åˆ¶")
        print(f"å¹¶å‘å¤„ç†{total_articles}ç¯‡æ–‡ç« : {elapsed:.4f}ç§’")


class TestMarkdownGenerationPerformance(unittest.TestCase):
    """Markdownç”Ÿæˆæ€§èƒ½æµ‹è¯•"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()

    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        import shutil
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def generate_article_data(self, content_size: str = "medium") -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ–‡ç« æ•°æ®"""
        if content_size == "small":
            content = "è¿™æ˜¯çŸ­å†…å®¹ã€‚" * 10
        elif content_size == "medium":
            content = "è¿™æ˜¯ä¸­ç­‰é•¿åº¦çš„å†…å®¹ã€‚" * 100
        elif content_size == "large":
            content = "è¿™æ˜¯å¾ˆé•¿çš„å†…å®¹ï¼ŒåŒ…å«å„ç§ä¿¡æ¯ã€‚" * 1000
        else:
            content = "è¶…é•¿å†…å®¹" * 10000

        return {
            "title": f"æµ‹è¯•æ–‡ç« -{content_size}",
            "author": "æµ‹è¯•ä½œè€…",
            "publish_date": "2024-01-15",
            "content": {"text": content, "html": f"<p>{content}</p>"},
            "images": [{"src": f"img{i}.jpg", "alt": f"å›¾{i}"} for i in range(5)],
            "original_url": "http://example.com/test"
        }

    def test_markdown_generation_scalability(self):
        """æµ‹è¯•Markdownç”Ÿæˆçš„å¯æ‰©å±•æ€§"""
        test_cases = [
            ("small", 0.01),
            ("medium", 0.05),
            ("large", 0.1),
            ("xlarge", 1.0)
        ]

        for size, max_time in test_cases:
            article_data = self.generate_article_data(size)

            start = time.perf_counter()
            result = generate_markdown(article_data)
            elapsed = time.perf_counter() - start

            self.assertTrue(result['success'])
            self.assertLess(elapsed, max_time,
                          f"{size}å†…å®¹ç”Ÿæˆè€—æ—¶{elapsed:.4f}ç§’ï¼Œè¶…è¿‡{max_time}ç§’é™åˆ¶")
            print(f"{size}å†…å®¹Markdownç”Ÿæˆ: {elapsed:.4f}ç§’")

    def test_batch_generation_performance(self):
        """æµ‹è¯•æ‰¹é‡ç”Ÿæˆæ€§èƒ½"""
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        articles = []
        for i in range(50):
            articles.append(self.generate_article_data("medium"))

        # ä¿å­˜åˆ°JSON
        json_file = os.path.join(self.temp_dir, 'batch.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(articles, f)

        start = time.perf_counter()
        results = batch_generate(json_file, self.temp_dir)
        elapsed = time.perf_counter() - start

        self.assertEqual(len(results), 50)
        self.assertLess(elapsed, 10.0,
                      f"æ‰¹é‡ç”Ÿæˆ50ç¯‡æ–‡ç« è€—æ—¶{elapsed:.4f}ç§’ï¼Œè¶…è¿‡10ç§’é™åˆ¶")
        print(f"æ‰¹é‡ç”Ÿæˆ50ç¯‡æ–‡ç« : {elapsed:.4f}ç§’")
        print(f"å¹³å‡æ¯ç¯‡: {elapsed/50:.4f}ç§’")

    def test_memory_usage_during_generation(self):
        """æµ‹è¯•ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨"""
        articles = []
        for i in range(100):
            articles.append(self.generate_article_data("large"))

        json_file = os.path.join(self.temp_dir, 'memory_test.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(articles, f)

        with memory_tracker():
            results = batch_generate(json_file, self.temp_dir)
            self.assertEqual(len(results), 100)


class TestEndToEndPerformance(unittest.TestCase):
    """ç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.temp_dir = tempfile.mkdtemp()

    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        import shutil
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def test_complete_pipeline_performance(self):
        """æµ‹è¯•å®Œæ•´ç®¡é“æ€§èƒ½"""
        # ç”Ÿæˆæµ‹è¯•HTML
        html = """
        <html>
            <h1 class="rich_media_title">å®Œæ•´ç®¡é“æµ‹è¯•æ–‡ç« </h1>
            <div id="js_content">
        """
        for i in range(100):
            html += f"<p>ç¬¬{i+1}æ®µå†…å®¹</p>\n"
            if i % 10 == 0:
                html += f'<img src="img{i}.jpg" alt="å›¾{i}">\n'
        html += "</div></html>"

        total_start = time.perf_counter()

        # æ­¥éª¤1: æå–
        with timer("HTMLæå–"):
            extracted = extract_from_html(html, save_images=False)

        # æ­¥éª¤2: çŠ¶æ€ç®¡ç†
        state_file = os.path.join(self.temp_dir, 'state.json')
        state_manager = ArticleStateManager(state_file)

        with timer("çŠ¶æ€è®°å½•"):
            state_manager.add_article("http://example.com/test", extracted)

        # æ­¥éª¤3: Markdownç”Ÿæˆ
        with timer("Markdownç”Ÿæˆ"):
            result = generate_markdown(extracted, Path(self.temp_dir))

        total_elapsed = time.perf_counter() - total_start

        self.assertTrue(result['success'])
        self.assertLess(total_elapsed, 2.0,
                      f"å®Œæ•´ç®¡é“è€—æ—¶{total_elapsed:.4f}ç§’ï¼Œè¶…è¿‡2ç§’é™åˆ¶")
        print(f"å®Œæ•´ç®¡é“æ€»è€—æ—¶: {total_elapsed:.4f}ç§’")

    def test_parallel_processing(self):
        """æµ‹è¯•å¹¶è¡Œå¤„ç†æ€§èƒ½"""
        def process_article(article_id: int):
            """å¤„ç†å•ç¯‡æ–‡ç« """
            html = f"""
            <html>
                <h1 class="rich_media_title">æ–‡ç« {article_id}</h1>
                <div id="js_content">
                    <p>å†…å®¹{article_id}</p>
                </div>
            </html>
            """
            extracted = extract_from_html(html, save_images=False)
            result = generate_markdown(extracted)
            return result['success']

        # ä¸²è¡Œå¤„ç†
        serial_start = time.perf_counter()
        for i in range(10):
            process_article(i)
        serial_time = time.perf_counter() - serial_start

        # å¹¶è¡Œå¤„ç†
        parallel_start = time.perf_counter()
        with multiprocessing.Pool(processes=4) as pool:
            results = pool.map(process_article, range(10))
        parallel_time = time.perf_counter() - parallel_start

        print(f"ä¸²è¡Œå¤„ç†10ç¯‡æ–‡ç« : {serial_time:.4f}ç§’")
        print(f"å¹¶è¡Œå¤„ç†10ç¯‡æ–‡ç« : {parallel_time:.4f}ç§’")
        print(f"åŠ é€Ÿæ¯”: {serial_time/parallel_time:.2f}x")

        # å¹¶è¡Œåº”è¯¥æ›´å¿«
        self.assertLess(parallel_time, serial_time)


class TestResourceUsage(unittest.TestCase):
    """èµ„æºä½¿ç”¨æµ‹è¯•"""

    def test_cpu_usage_pattern(self):
        """æµ‹è¯•CPUä½¿ç”¨æ¨¡å¼"""
        import psutil
        import os

        # è·å–å½“å‰è¿›ç¨‹
        process = psutil.Process(os.getpid())

        # è®°å½•åˆå§‹CPUä½¿ç”¨
        initial_cpu = process.cpu_percent(interval=1)

        # æ‰§è¡Œå¯†é›†æ“ä½œ
        for i in range(100):
            text = "æµ‹è¯•æ–‡æœ¬" * 1000
            clean_text(text)
            sanitize_slug(f"æ ‡é¢˜{i}")
            generate_tags(text, f"æ ‡é¢˜{i}")

        # è®°å½•ç»“æŸCPUä½¿ç”¨
        final_cpu = process.cpu_percent(interval=1)

        print(f"åˆå§‹CPUä½¿ç”¨: {initial_cpu}%")
        print(f"ç»“æŸCPUä½¿ç”¨: {final_cpu}%")

        # CPUä½¿ç”¨åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        self.assertLess(final_cpu, 80, "CPUä½¿ç”¨ç‡è¿‡é«˜")

    def test_memory_leak_detection(self):
        """å†…å­˜æ³„æ¼æ£€æµ‹"""
        import gc
        import psutil
        import os

        process = psutil.Process(os.getpid())

        # è®°å½•åˆå§‹å†…å­˜
        gc.collect()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # æ‰§è¡Œå¤šæ¬¡æ“ä½œ
        for iteration in range(10):
            # åˆ›å»ºå’Œå¤„ç†å¤§é‡æ•°æ®
            articles = []
            for i in range(100):
                article = {
                    "title": f"æ–‡ç« {i}",
                    "content": {"text": "å†…å®¹" * 1000}
                }
                articles.append(article)

            # å¤„ç†æ•°æ®
            for article in articles:
                generate_markdown(article)

            # æ¸…ç†
            articles = None
            gc.collect()

        # è®°å½•æœ€ç»ˆå†…å­˜
        final_memory = process.memory_info().rss / 1024 / 1024  # MB

        memory_increase = final_memory - initial_memory
        print(f"åˆå§‹å†…å­˜: {initial_memory:.2f} MB")
        print(f"æœ€ç»ˆå†…å­˜: {final_memory:.2f} MB")
        print(f"å†…å­˜å¢é•¿: {memory_increase:.2f} MB")

        # å†…å­˜å¢é•¿åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆä¸è¶…è¿‡100MBï¼‰
        self.assertLess(memory_increase, 100,
                      f"å†…å­˜æ³„æ¼æ£€æµ‹ï¼šå†…å­˜å¢é•¿{memory_increase:.2f}MBè¶…è¿‡é™åˆ¶")


class TestLoadTesting(unittest.TestCase):
    """è´Ÿè½½æµ‹è¯•"""

    def test_sustained_load(self):
        """æŒç»­è´Ÿè½½æµ‹è¯•"""
        duration = 10  # æµ‹è¯•æŒç»­10ç§’
        operations_count = 0
        errors_count = 0
        start_time = time.time()

        while time.time() - start_time < duration:
            try:
                # æ‰§è¡Œæ“ä½œ
                text = "æµ‹è¯•å†…å®¹" * random.randint(10, 100)
                clean_text(text)
                sanitize_slug(f"æ ‡é¢˜{operations_count}")
                operations_count += 1
            except Exception as e:
                errors_count += 1

        elapsed = time.time() - start_time
        ops_per_second = operations_count / elapsed

        print(f"æŒç»­è´Ÿè½½æµ‹è¯•ç»“æœ:")
        print(f"  æŒç»­æ—¶é—´: {elapsed:.2f}ç§’")
        print(f"  æ€»æ“ä½œæ•°: {operations_count}")
        print(f"  é”™è¯¯æ•°: {errors_count}")
        print(f"  æ“ä½œ/ç§’: {ops_per_second:.2f}")

        # åº”è¯¥èƒ½å¤Ÿç¨³å®šå¤„ç†
        self.assertEqual(errors_count, 0, "æŒç»­è´Ÿè½½ä¸‹å‡ºç°é”™è¯¯")
        self.assertGreater(ops_per_second, 100, "å¤„ç†é€Ÿåº¦è¿‡æ…¢")

    def test_spike_load(self):
        """çªå‘è´Ÿè½½æµ‹è¯•"""
        # æ¨¡æ‹Ÿçªå‘è¯·æ±‚
        spike_size = 100
        articles = []

        for i in range(spike_size):
            articles.append({
                "title": f"çªå‘æ–‡ç« {i}",
                "content": {"text": "å†…å®¹" * random.randint(50, 200)}
            })

        start = time.perf_counter()
        results = []
        for article in articles:
            result = generate_markdown(article)
            results.append(result['success'])
        elapsed = time.perf_counter() - start

        success_rate = sum(results) / len(results)
        print(f"çªå‘è´Ÿè½½æµ‹è¯• ({spike_size}ç¯‡æ–‡ç« ):")
        print(f"  è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"  æˆåŠŸç‡: {success_rate * 100:.2f}%")
        print(f"  å¹³å‡å¤„ç†æ—¶é—´: {elapsed/spike_size:.4f}ç§’")

        # æ‰€æœ‰è¯·æ±‚éƒ½åº”è¯¥æˆåŠŸ
        self.assertEqual(success_rate, 1.0, "çªå‘è´Ÿè½½ä¸‹æœ‰å¤±è´¥è¯·æ±‚")
        # åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        self.assertLess(elapsed, 30, f"å¤„ç†{spike_size}ç¯‡æ–‡ç« è¶…è¿‡30ç§’")


if __name__ == '__main__':
    # è¿è¡Œæµ‹è¯•
    unittest.main(verbosity=2)