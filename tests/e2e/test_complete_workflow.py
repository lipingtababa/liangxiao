#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯æµ‹è¯• - å®Œæ•´å·¥ä½œæµ
æµ‹è¯•ä»æ–‡ç« URLåˆ°æœ€ç»ˆMarkdownæ–‡ä»¶çš„å®Œæ•´æµç¨‹
"""

import unittest
from unittest.mock import Mock, patch, MagicMock
import sys
import json
import tempfile
import os
import time
from pathlib import Path
from datetime import datetime, timedelta
import shutil

# æ·»åŠ scriptsç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'scripts'))

from wechat_extractor import extract_from_url
from state_manager import ArticleStateManager
from markdown_generator import generate_markdown, batch_generate


class TestCompleteWorkflow(unittest.TestCase):
    """å®Œæ•´å·¥ä½œæµç«¯åˆ°ç«¯æµ‹è¯•"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.test_dir = tempfile.mkdtemp()
        self.posts_dir = Path(self.test_dir) / 'posts'
        self.images_dir = Path(self.test_dir) / 'images'
        self.state_file = Path(self.test_dir) / 'state.json'
        self.posts_dir.mkdir(parents=True, exist_ok=True)
        self.images_dir.mkdir(parents=True, exist_ok=True)

    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    @patch('wechat_extractor.requests.get')
    def test_single_article_workflow(self, mock_get):
        """æµ‹è¯•å•ç¯‡æ–‡ç« çš„å®Œæ•´å¤„ç†æµç¨‹"""
        # æ¨¡æ‹Ÿå¾®ä¿¡æ–‡ç« HTML
        html_content = """
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <meta property="og:title" content="2024å¹´ç‘å…¸ç§»æ°‘æ–°æ”¿ç­–è§£è¯»">
            <meta property="og:description" content="è¯¦ç»†è§£è¯»2024å¹´ç‘å…¸ç§»æ°‘æ”¿ç­–çš„æœ€æ–°å˜åŒ–">
            <meta name="author" content="ç‘å…¸é©¬å·¥">
        </head>
        <body>
            <div class="rich_media">
                <h1 class="rich_media_title">2024å¹´ç‘å…¸ç§»æ°‘æ–°æ”¿ç­–è§£è¯»</h1>
                <div class="rich_media_meta_list">
                    <span class="rich_media_meta rich_media_meta_nickname">ç‘å…¸é©¬å·¥</span>
                    <em id="publish_time">2024-01-15</em>
                </div>
                <div id="js_content" class="rich_media_content">
                    <p>2024å¹´ï¼Œç‘å…¸æ”¿åºœå¯¹ç§»æ°‘æ”¿ç­–è¿›è¡Œäº†é‡å¤§è°ƒæ•´ã€‚</p>
                    <p>é¦–å…ˆï¼Œå·¥ä½œç­¾è¯çš„è¦æ±‚æœ‰æ‰€æé«˜ã€‚ç”³è¯·äººéœ€è¦æœ‰æ›´é«˜çš„å·¥èµ„æ°´å¹³ã€‚</p>
                    <p>å…¶æ¬¡ï¼Œæ°¸ä¹…å±…ç•™æƒçš„ç”³è¯·æ¡ä»¶ä¹Ÿå‘ç”Ÿäº†å˜åŒ–ã€‚</p>
                    <img data-src="http://example.com/policy.jpg" alt="æ”¿ç­–å›¾è¡¨">
                    <p>å¯¹äºç•™å­¦ç”Ÿæ¥è¯´ï¼Œæ¯•ä¸šåæ‰¾å·¥ä½œçš„æ—¶é—´å»¶é•¿åˆ°äº†12ä¸ªæœˆã€‚</p>
                    <p>å®¶åº­å›¢èšçš„æ¡ä»¶ä¹Ÿæœ‰æ‰€æ”¾å®½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæœ‰å­©å­çš„å®¶åº­ã€‚</p>
                    <img data-src="http://example.com/sweden.jpg" alt="ç‘å…¸é£æ™¯">
                </div>
            </div>
        </body>
        </html>
        """

        # è®¾ç½®mockå“åº”
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.text = html_content
        mock_response.apparent_encoding = 'utf-8'
        mock_response.raise_for_status = Mock()
        mock_get.return_value = mock_response

        # æ­¥éª¤1: æå–æ–‡ç« å†…å®¹
        url = "http://mp.weixin.qq.com/s/abc123"
        extracted_data = extract_from_url(url, save_images=False, image_dir=self.images_dir)

        # éªŒè¯æå–ç»“æœ
        self.assertEqual(extracted_data['title'], "2024å¹´ç‘å…¸ç§»æ°‘æ–°æ”¿ç­–è§£è¯»")
        self.assertEqual(extracted_data['author'], "ç‘å…¸é©¬å·¥")
        self.assertEqual(extracted_data['publish_date'], "2024-01-15")
        self.assertIn("ç§»æ°‘æ”¿ç­–", extracted_data['content']['text'])
        self.assertEqual(len(extracted_data['images']), 2)

        # æ­¥éª¤2: ä½¿ç”¨çŠ¶æ€ç®¡ç†å™¨è®°å½•å¤„ç†çŠ¶æ€
        state_manager = ArticleStateManager(str(self.state_file))
        state_manager.add_article(url, extracted_data)

        # éªŒè¯çŠ¶æ€è®°å½•
        self.assertTrue(state_manager.is_article_processed(url))
        article_state = state_manager.get_article_state(url)
        self.assertEqual(article_state['status'], 'completed')
        self.assertEqual(article_state['title'], "2024å¹´ç‘å…¸ç§»æ°‘æ–°æ”¿ç­–è§£è¯»")

        # æ­¥éª¤3: ç”ŸæˆMarkdownæ–‡ä»¶
        markdown_result = generate_markdown(extracted_data, self.posts_dir)

        # éªŒè¯Markdownç”Ÿæˆ
        self.assertTrue(markdown_result['success'])
        self.assertIsNotNone(markdown_result['file_path'])
        self.assertTrue(Path(markdown_result['file_path']).exists())

        # æ­¥éª¤4: éªŒè¯ç”Ÿæˆçš„Markdownå†…å®¹
        with open(markdown_result['file_path'], 'r', encoding='utf-8') as f:
            markdown_content = f.read()

        # éªŒè¯frontmatter
        self.assertIn("---", markdown_content)
        self.assertIn("title: 2024å¹´ç‘å…¸ç§»æ°‘æ–°æ”¿ç­–è§£è¯»", markdown_content)
        self.assertIn("date: '2024-01-15'", markdown_content)
        self.assertIn("author: ç‘å…¸é©¬å·¥", markdown_content)
        self.assertIn("category:", markdown_content)
        self.assertIn("tags:", markdown_content)

        # éªŒè¯å†…å®¹
        self.assertIn("# 2024å¹´ç‘å…¸ç§»æ°‘æ–°æ”¿ç­–è§£è¯»", markdown_content)
        self.assertIn("ç§»æ°‘æ”¿ç­–", markdown_content)
        self.assertIn("å·¥ä½œç­¾è¯", markdown_content)
        self.assertIn("æ°¸ä¹…å±…ç•™æƒ", markdown_content)
        self.assertIn("ç•™å­¦ç”Ÿ", markdown_content)

        # éªŒè¯å›¾ç‰‡å¼•ç”¨
        self.assertIn("![æ”¿ç­–å›¾è¡¨]", markdown_content)
        self.assertIn("![ç‘å…¸é£æ™¯]", markdown_content)

        # éªŒè¯åŸæ–‡é“¾æ¥
        self.assertIn("åŸæ–‡é“¾æ¥", markdown_content)
        self.assertIn(url, markdown_content)

        # æ­¥éª¤5: éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        stats = state_manager.get_statistics()
        self.assertEqual(stats['total_articles'], 1)
        self.assertEqual(stats['successful_articles'], 1)
        self.assertEqual(stats['total_processed'], 1)

    @patch('wechat_extractor.requests.get')
    def test_batch_processing_workflow(self, mock_get):
        """æµ‹è¯•æ‰¹é‡å¤„ç†å¤šç¯‡æ–‡ç« çš„å·¥ä½œæµ"""
        # å‡†å¤‡å¤šä¸ªæ–‡ç« çš„HTMLå“åº”
        articles_html = [
            """
            <html>
                <h1 class="rich_media_title">ç‘å…¸æ•™è‚²ä½“ç³»ä»‹ç»</h1>
                <em id="publish_time">2024-01-10</em>
                <div id="js_content">
                    <p>ç‘å…¸çš„æ•™è‚²ä½“ç³»éå¸¸å®Œå–„ï¼Œä»å¹¼å„¿å›­åˆ°å¤§å­¦éƒ½æ˜¯å…è´¹çš„ã€‚</p>
                </div>
            </html>
            """,
            """
            <html>
                <h1 class="rich_media_title">æ–¯å¾·å“¥å°”æ‘©ç¾é£Ÿæ¨è</h1>
                <em id="publish_time">2024-01-12</em>
                <div id="js_content">
                    <p>æ–¯å¾·å“¥å°”æ‘©æœ‰å¾ˆå¤šå€¼å¾—å°è¯•çš„é¤å…å’Œç¾é£Ÿã€‚</p>
                </div>
            </html>
            """,
            """
            <html>
                <h1 class="rich_media_title">ç‘å…¸èŒåœºæ–‡åŒ–è§£æ</h1>
                <em id="publish_time">2024-01-14</em>
                <div id="js_content">
                    <p>ç‘å…¸çš„èŒåœºæ–‡åŒ–å¼ºè°ƒå¹³ç­‰ã€å·¥ä½œä¸ç”Ÿæ´»çš„å¹³è¡¡ã€‚</p>
                </div>
            </html>
            """
        ]

        urls = [
            "http://mp.weixin.qq.com/s/edu123",
            "http://mp.weixin.qq.com/s/food456",
            "http://mp.weixin.qq.com/s/work789"
        ]

        # è®¾ç½®mockå“åº”
        mock_responses = []
        for html in articles_html:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.text = html
            mock_response.apparent_encoding = 'utf-8'
            mock_response.raise_for_status = Mock()
            mock_responses.append(mock_response)

        mock_get.side_effect = mock_responses

        # åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
        state_manager = ArticleStateManager(str(self.state_file))

        # å¤„ç†æ‰€æœ‰æ–‡ç« 
        extracted_articles = []
        for url in urls:
            extracted_data = extract_from_url(url, save_images=False)
            extracted_articles.append(extracted_data)
            state_manager.add_article(url, extracted_data)

        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        json_file = Path(self.test_dir) / 'articles.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(extracted_articles, f, ensure_ascii=False, indent=2)

        # æ‰¹é‡ç”ŸæˆMarkdown
        results = batch_generate(str(json_file), str(self.posts_dir))

        # éªŒè¯æ‰¹é‡å¤„ç†ç»“æœ
        self.assertEqual(len(results), 3)
        for result in results:
            self.assertTrue(result['success'])
            self.assertTrue(Path(result['file_path']).exists())

        # éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
        markdown_files = list(self.posts_dir.glob('*.md'))
        self.assertEqual(len(markdown_files), 3)

        # éªŒè¯æ–‡ä»¶å†…å®¹åŒ…å«æ­£ç¡®çš„æ ‡é¢˜
        titles = ["ç‘å…¸æ•™è‚²ä½“ç³»ä»‹ç»", "æ–¯å¾·å“¥å°”æ‘©ç¾é£Ÿæ¨è", "ç‘å…¸èŒåœºæ–‡åŒ–è§£æ"]
        for title in titles:
            found = False
            for md_file in markdown_files:
                with open(md_file, 'r', encoding='utf-8') as f:
                    if title in f.read():
                        found = True
                        break
            self.assertTrue(found, f"æœªæ‰¾åˆ°æ ‡é¢˜: {title}")

        # éªŒè¯çŠ¶æ€ç®¡ç†å™¨ç»Ÿè®¡
        stats = state_manager.get_statistics()
        self.assertEqual(stats['total_articles'], 3)
        self.assertEqual(stats['successful_articles'], 3)

    @patch('wechat_extractor.requests.get')
    def test_incremental_update_workflow(self, mock_get):
        """æµ‹è¯•å¢é‡æ›´æ–°å·¥ä½œæµ"""
        # åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
        state_manager = ArticleStateManager(str(self.state_file))

        # ç¬¬ä¸€æ¬¡å¤„ç†
        url = "http://mp.weixin.qq.com/s/update123"
        original_html = """
        <html>
            <h1 class="rich_media_title">åŸå§‹æ ‡é¢˜</h1>
            <div id="js_content"><p>åŸå§‹å†…å®¹</p></div>
        </html>
        """

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.text = original_html
        mock_response.apparent_encoding = 'utf-8'
        mock_response.raise_for_status = Mock()
        mock_get.return_value = mock_response

        # ç¬¬ä¸€æ¬¡æå–å’Œå¤„ç†
        extracted_data = extract_from_url(url, save_images=False)
        state_manager.add_article(url, extracted_data)

        # è®°å½•ç¬¬ä¸€æ¬¡å¤„ç†çš„å†…å®¹å“ˆå¸Œ
        first_state = state_manager.get_article_state(url)
        first_hash = first_state['content_hash']

        # æ¨¡æ‹Ÿæ–‡ç« æ›´æ–°
        updated_html = """
        <html>
            <h1 class="rich_media_title">æ›´æ–°åçš„æ ‡é¢˜</h1>
            <div id="js_content"><p>æ›´æ–°åçš„å†…å®¹ï¼Œæ·»åŠ äº†æ–°ä¿¡æ¯</p></div>
        </html>
        """

        mock_response.text = updated_html

        # æ£€æµ‹æ˜¯å¦éœ€è¦æ›´æ–°
        self.assertTrue(state_manager.needs_update(url, "æ›´æ–°åçš„å†…å®¹ï¼Œæ·»åŠ äº†æ–°ä¿¡æ¯"))

        # ç¬¬äºŒæ¬¡æå–å’Œå¤„ç†
        updated_data = extract_from_url(url, save_images=False)
        state_manager.add_article(url, updated_data)

        # éªŒè¯æ›´æ–°
        updated_state = state_manager.get_article_state(url)
        self.assertEqual(updated_state['title'], "æ›´æ–°åçš„æ ‡é¢˜")
        self.assertNotEqual(updated_state['content_hash'], first_hash)
        self.assertEqual(updated_state['process_count'], 2)

        # ç”Ÿæˆæ›´æ–°åçš„Markdown
        markdown_result = generate_markdown(updated_data, self.posts_dir)
        self.assertTrue(markdown_result['success'])

        # éªŒè¯MarkdownåŒ…å«æ›´æ–°çš„å†…å®¹
        with open(markdown_result['file_path'], 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn("æ›´æ–°åçš„æ ‡é¢˜", content)
            self.assertIn("æ›´æ–°åçš„å†…å®¹", content)

    @patch('wechat_extractor.requests.get')
    def test_error_recovery_workflow(self, mock_get):
        """æµ‹è¯•é”™è¯¯æ¢å¤å·¥ä½œæµ"""
        state_manager = ArticleStateManager(str(self.state_file))
        url = "http://mp.weixin.qq.com/s/error123"

        # ç¬¬ä¸€æ¬¡è¯·æ±‚å¤±è´¥
        mock_get.side_effect = Exception("ç½‘ç»œé”™è¯¯")

        # æå–å¤±è´¥
        extracted_data = extract_from_url(url, save_images=False)
        self.assertIn('error', extracted_data)

        # è®°å½•é”™è¯¯çŠ¶æ€
        state_manager.mark_article_error(url, extracted_data['error'])

        # éªŒè¯é”™è¯¯çŠ¶æ€
        error_state = state_manager.get_article_state(url)
        self.assertEqual(error_state['status'], 'error')

        # ç¬¬äºŒæ¬¡è¯·æ±‚æˆåŠŸ
        mock_get.side_effect = None
        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.text = """
        <html>
            <h1 class="rich_media_title">æ¢å¤åçš„æ–‡ç« </h1>
            <div id="js_content"><p>æˆåŠŸè·å–å†…å®¹</p></div>
        </html>
        """
        mock_response.apparent_encoding = 'utf-8'
        mock_response.raise_for_status = Mock()
        mock_get.return_value = mock_response

        # é‡æ–°æå–
        extracted_data = extract_from_url(url, save_images=False)
        self.assertNotIn('error', extracted_data)
        self.assertEqual(extracted_data['title'], "æ¢å¤åçš„æ–‡ç« ")

        # æ›´æ–°çŠ¶æ€
        state_manager.add_article(url, extracted_data)

        # éªŒè¯çŠ¶æ€æ¢å¤
        recovered_state = state_manager.get_article_state(url)
        self.assertEqual(recovered_state['status'], 'completed')
        self.assertEqual(recovered_state['title'], "æ¢å¤åçš„æ–‡ç« ")

        # ç”ŸæˆMarkdown
        markdown_result = generate_markdown(extracted_data, self.posts_dir)
        self.assertTrue(markdown_result['success'])


class TestPerformanceAndScalability(unittest.TestCase):
    """æ€§èƒ½å’Œå¯æ‰©å±•æ€§æµ‹è¯•"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.test_dir = tempfile.mkdtemp()
        self.posts_dir = Path(self.test_dir) / 'posts'
        self.posts_dir.mkdir(parents=True, exist_ok=True)

    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def test_large_batch_processing(self):
        """æµ‹è¯•å¤§æ‰¹é‡æ–‡ç« å¤„ç†æ€§èƒ½"""
        # ç”Ÿæˆ100ç¯‡æµ‹è¯•æ–‡ç« 
        articles = []
        for i in range(100):
            article = {
                "title": f"æµ‹è¯•æ–‡ç«  {i+1}",
                "author": "æµ‹è¯•ä½œè€…",
                "publish_date": (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d"),
                "content": {"text": f"è¿™æ˜¯ç¬¬{i+1}ç¯‡æ–‡ç« çš„å†…å®¹" * 100},  # è¾ƒé•¿å†…å®¹
                "images": [{"src": f"http://example.com/img{j}.jpg", "alt": f"å›¾ç‰‡{j}"}
                          for j in range(5)],  # æ¯ç¯‡5å¼ å›¾ç‰‡
                "original_url": f"http://mp.weixin.qq.com/s/test{i}"
            }
            articles.append(article)

        # ä¿å­˜åˆ°JSON
        json_file = Path(self.test_dir) / 'large_batch.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

        # è®¡æ—¶å¼€å§‹
        start_time = time.time()

        # æ‰¹é‡å¤„ç†
        results = batch_generate(str(json_file), str(self.posts_dir))

        # è®¡æ—¶ç»“æŸ
        end_time = time.time()
        processing_time = end_time - start_time

        # éªŒè¯ç»“æœ
        self.assertEqual(len(results), 100)
        success_count = sum(1 for r in results if r['success'])
        self.assertEqual(success_count, 100)

        # æ€§èƒ½è¦æ±‚ï¼š100ç¯‡æ–‡ç« åº”è¯¥åœ¨30ç§’å†…å®Œæˆ
        self.assertLess(processing_time, 30, f"å¤„ç†100ç¯‡æ–‡ç« è€—æ—¶{processing_time:.2f}ç§’ï¼Œè¶…è¿‡30ç§’é™åˆ¶")

        # éªŒè¯æ–‡ä»¶ç”Ÿæˆ
        markdown_files = list(self.posts_dir.glob('*.md'))
        self.assertEqual(len(markdown_files), 100)

        print(f"å¤„ç†100ç¯‡æ–‡ç« è€—æ—¶: {processing_time:.2f}ç§’")
        print(f"å¹³å‡æ¯ç¯‡: {processing_time/100:.3f}ç§’")

    def test_concurrent_state_management(self):
        """æµ‹è¯•å¹¶å‘çŠ¶æ€ç®¡ç†"""
        import threading
        import random

        state_file = Path(self.test_dir) / 'concurrent_state.json'

        def worker(thread_id, urls):
            """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
            state_manager = ArticleStateManager(str(state_file))
            for url in urls:
                article_data = {
                    "title": f"çº¿ç¨‹{thread_id}æ–‡ç« ",
                    "content": {"text": f"å†…å®¹-{random.random()}"}
                }
                state_manager.add_article(url, article_data)
                time.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†
        threads = []
        urls_per_thread = 10
        thread_count = 5

        for i in range(thread_count):
            urls = [f"http://example.com/thread{i}/article{j}"
                   for j in range(urls_per_thread)]
            thread = threading.Thread(target=worker, args=(i, urls))
            threads.append(thread)

        # å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
        for thread in threads:
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        # éªŒè¯æœ€ç»ˆçŠ¶æ€
        final_state_manager = ArticleStateManager(str(state_file))
        stats = final_state_manager.get_statistics()

        # åº”è¯¥å¤„ç†äº†æ‰€æœ‰æ–‡ç« 
        expected_total = thread_count * urls_per_thread
        self.assertEqual(stats['total_articles'], expected_total)
        self.assertEqual(stats['successful_articles'], expected_total)

    def test_memory_efficiency(self):
        """æµ‹è¯•å†…å­˜æ•ˆç‡"""
        import tracemalloc

        # å¼€å§‹è·Ÿè¸ªå†…å­˜
        tracemalloc.start()

        # åˆ›å»ºå¤§é‡æ–‡ç« æ•°æ®
        articles = []
        for i in range(1000):
            article = {
                "title": f"æ–‡ç« {i}",
                "content": {"text": "å†…å®¹" * 1000},  # è¾ƒé•¿å†…å®¹
                "images": [{"src": f"img{j}.jpg", "alt": f"å›¾{j}"} for j in range(10)]
            }
            articles.append(article)

        # è·å–å†…å­˜ä½¿ç”¨
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        # è½¬æ¢ä¸ºMB
        current_mb = current / 1024 / 1024
        peak_mb = peak / 1024 / 1024

        print(f"å½“å‰å†…å­˜ä½¿ç”¨: {current_mb:.2f} MB")
        print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {peak_mb:.2f} MB")

        # å†…å­˜ä½¿ç”¨åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆå°äº500MBï¼‰
        self.assertLess(peak_mb, 500, f"å³°å€¼å†…å­˜ä½¿ç”¨{peak_mb:.2f}MBè¶…è¿‡500MBé™åˆ¶")


class TestDataIntegrity(unittest.TestCase):
    """æ•°æ®å®Œæ•´æ€§æµ‹è¯•"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.test_dir = tempfile.mkdtemp()

    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)

    def test_unicode_handling(self):
        """æµ‹è¯•Unicodeå­—ç¬¦å¤„ç†"""
        # åŒ…å«å„ç§Unicodeå­—ç¬¦çš„æ–‡ç« 
        article_data = {
            "title": "æµ‹è¯•ğŸ‰ Emojiå’Œç‰¹æ®Šå­—ç¬¦ Â© Â® â„¢ â‚¬ Â¥",
            "author": "ä½œè€…å with English",
            "content": {
                "text": "ä¸­æ–‡å†…å®¹ with English. åŒ…å«emojiğŸ˜Š å’Œç‰¹æ®Šç¬¦å· â†’ â† â†‘ â†“"
            },
            "original_url": "http://example.com/unicode"
        }

        # ç”ŸæˆMarkdown
        posts_dir = Path(self.test_dir) / 'posts'
        posts_dir.mkdir(parents=True, exist_ok=True)
        result = generate_markdown(article_data, posts_dir)

        self.assertTrue(result['success'])

        # éªŒè¯æ–‡ä»¶å†…å®¹
        with open(result['file_path'], 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn("ğŸ‰", content)
            self.assertIn("ğŸ˜Š", content)
            self.assertIn("Â©", content)
            self.assertIn("â†’", content)

    def test_data_consistency_across_pipeline(self):
        """æµ‹è¯•æ•°æ®åœ¨æ•´ä¸ªç®¡é“ä¸­çš„ä¸€è‡´æ€§"""
        original_data = {
            "title": "åŸå§‹æ ‡é¢˜",
            "author": "åŸå§‹ä½œè€…",
            "publish_date": "2024-01-15",
            "content": {"text": "åŸå§‹å†…å®¹"},
            "images": [{"src": "img1.jpg", "alt": "å›¾ç‰‡1"}],
            "original_url": "http://example.com/original"
        }

        # é€šè¿‡çŠ¶æ€ç®¡ç†å™¨
        state_file = Path(self.test_dir) / 'state.json'
        state_manager = ArticleStateManager(str(state_file))
        state_manager.add_article(original_data['original_url'], original_data)

        # ä»çŠ¶æ€ç®¡ç†å™¨è·å–
        stored_state = state_manager.get_article_state(original_data['original_url'])

        # ç”ŸæˆMarkdown
        posts_dir = Path(self.test_dir) / 'posts'
        posts_dir.mkdir(parents=True, exist_ok=True)
        markdown_result = generate_markdown(original_data, posts_dir)

        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        self.assertEqual(stored_state['title'], original_data['title'])
        self.assertEqual(stored_state['author'], original_data['author'])

        # éªŒè¯Markdownä¸­çš„æ•°æ®
        with open(markdown_result['file_path'], 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn(original_data['title'], content)
            self.assertIn(original_data['author'], content)
            self.assertIn(original_data['content']['text'], content)

    def test_cleanup_old_data(self):
        """æµ‹è¯•æ—§æ•°æ®æ¸…ç†åŠŸèƒ½"""
        state_file = Path(self.test_dir) / 'state.json'
        state_manager = ArticleStateManager(str(state_file))

        # æ·»åŠ ä¸åŒæ—¶é—´çš„æ–‡ç« 
        now = datetime.now()

        # æ·»åŠ æ—§æ–‡ç« ï¼ˆ40å¤©å‰ï¼‰
        old_article = {
            "title": "æ—§æ–‡ç« ",
            "content": {"text": "æ—§å†…å®¹"}
        }
        state_manager.add_article("http://example.com/old", old_article)

        # æ‰‹åŠ¨ä¿®æ”¹æ—¶é—´æˆ³
        state_manager.state_data['articles']['http://example.com/old']['last_processed_at'] = \
            (now - timedelta(days=40)).isoformat()
        state_manager._save_state()

        # æ·»åŠ æ–°æ–‡ç« ï¼ˆ5å¤©å‰ï¼‰
        recent_article = {
            "title": "æ–°æ–‡ç« ",
            "content": {"text": "æ–°å†…å®¹"}
        }
        state_manager.add_article("http://example.com/recent", recent_article)
        state_manager.state_data['articles']['http://example.com/recent']['last_processed_at'] = \
            (now - timedelta(days=5)).isoformat()
        state_manager._save_state()

        # æ¸…ç†30å¤©å‰çš„æ•°æ®
        removed_count = state_manager.cleanup_old_entries(30)

        # éªŒè¯æ¸…ç†ç»“æœ
        self.assertEqual(removed_count, 1)
        self.assertFalse(state_manager.is_article_processed("http://example.com/old"))
        self.assertTrue(state_manager.is_article_processed("http://example.com/recent"))


if __name__ == '__main__':
    # è¿è¡Œæµ‹è¯•
    unittest.main(verbosity=2)